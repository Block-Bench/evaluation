{
  "snapshot_timestamp": "2025-12-18T19:05:19.380096",
  "sample_count": 45,
  "metrics": {
    "total_samples": 45,
    "vulnerable_samples": 45,
    "safe_samples": 0,
    "detection": {
      "accuracy": 0.8666666666666667,
      "precision": 1.0,
      "recall": 0.8666666666666667,
      "f1": 0.9285714285714286,
      "f2": 0.8904109589041096,
      "fpr": 0,
      "fnr": 0.13333333333333333,
      "tp": 39,
      "tn": 0,
      "fp": 0,
      "fn": 6
    },
    "target_finding": {
      "target_detection_rate": 0.4666666666666667,
      "lucky_guess_rate": 0.48717948717948717,
      "bonus_discovery_rate": 0.6222222222222222,
      "target_found_count": 21,
      "lucky_guess_count": 19
    },
    "finding_quality": {
      "finding_precision": 0.4846153846153846,
      "invalid_rate": 0.5153846153846153,
      "hallucination_rate": 0.007692307692307693,
      "over_flagging_score": 1.488888888888889,
      "avg_findings_per_sample": 2.888888888888889,
      "total_findings": 130,
      "valid_findings": 63,
      "invalid_findings": 67,
      "hallucinated_findings": 1
    },
    "reasoning_quality": {
      "mean_rcir": 0.9119047619047619,
      "mean_ava": 0.9071428571428571,
      "mean_fsv": 0.8666666666666666,
      "std_rcir": 0.11639911011028872,
      "std_ava": 0.1416616645775573,
      "std_fsv": 0.2407000372590498,
      "n_samples_with_reasoning": 21
    },
    "type_accuracy": {
      "exact_match_rate": 0.6190476190476191,
      "semantic_match_rate": 0.9047619047619048,
      "partial_match_rate": 0.09523809523809523,
      "n_samples": 21
    },
    "calibration": {
      "ece": 0.08000000000000004,
      "mce": 0.1000000000000002,
      "overconfidence_rate": 0.050000000000000044,
      "underconfidence_rate": 0.0,
      "brier_score": 0.05387499999999999,
      "n_samples": 40
    },
    "composite": {
      "true_understanding_score": 0.20246153846153847,
      "sui": 0.7277712378945256,
      "sui_components": {
        "f2": 0.8904109589041096,
        "target_detection": 0.4666666666666667,
        "finding_precision": 0.4846153846153846,
        "avg_reasoning": 0.8952380952380952,
        "calibration": 0.9199999999999999
      },
      "lucky_guess_indicator": 0.4
    },
    "by_prompt_type": {
      "direct": {
        "total_samples": 39,
        "vulnerable_samples": 39,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.9743589743589743,
          "precision": 1.0,
          "recall": 0.9743589743589743,
          "f1": 0.9870129870129869,
          "f2": 0.9793814432989689,
          "fpr": 0,
          "fnr": 0.02564102564102564,
          "tp": 38,
          "tn": 0,
          "fp": 0,
          "fn": 1
        },
        "target_finding": {
          "target_detection_rate": 0.48717948717948717,
          "lucky_guess_rate": 0.5,
          "bonus_discovery_rate": 0.717948717948718,
          "target_found_count": 19,
          "lucky_guess_count": 19
        },
        "finding_quality": {
          "finding_precision": 0.6931818181818182,
          "invalid_rate": 0.3068181818181818,
          "hallucination_rate": 0.011363636363636364,
          "over_flagging_score": 0.6923076923076923,
          "avg_findings_per_sample": 2.2564102564102564,
          "total_findings": 88,
          "valid_findings": 61,
          "invalid_findings": 27,
          "hallucinated_findings": 1
        },
        "reasoning_quality": {
          "mean_rcir": 0.9289473684210525,
          "mean_ava": 0.9368421052631579,
          "mean_fsv": 0.9052631578947368,
          "std_rcir": 0.10920259958550753,
          "std_ava": 0.10618021579911587,
          "std_fsv": 0.14858520224729585,
          "n_samples_with_reasoning": 19
        },
        "type_accuracy": {
          "exact_match_rate": 0.6842105263157895,
          "semantic_match_rate": 0.9473684210526315,
          "partial_match_rate": 0.05263157894736842,
          "n_samples": 19
        },
        "calibration": {
          "ece": 0.05641025641025646,
          "mce": 0.1000000000000002,
          "overconfidence_rate": 0.02564102564102566,
          "underconfidence_rate": 0.0,
          "brier_score": 0.02961538461538461,
          "n_samples": 39
        },
        "composite": {
          "true_understanding_score": 0.31193181818181814,
          "sui": 0.7958975323374401,
          "sui_components": {
            "f2": 0.9793814432989689,
            "target_detection": 0.48717948717948717,
            "finding_precision": 0.6931818181818182,
            "avg_reasoning": 0.9236842105263158,
            "calibration": 0.9435897435897436
          },
          "lucky_guess_indicator": 0.48717948717948717
        }
      },
      "naturalistic": {
        "total_samples": 4,
        "vulnerable_samples": 4,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.25,
          "precision": 1.0,
          "recall": 0.25,
          "f1": 0.4,
          "f2": 0.29411764705882354,
          "fpr": 0,
          "fnr": 0.75,
          "tp": 1,
          "tn": 0,
          "fp": 0,
          "fn": 3
        },
        "target_finding": {
          "target_detection_rate": 0.25,
          "lucky_guess_rate": 0.0,
          "bonus_discovery_rate": 0.0,
          "target_found_count": 1,
          "lucky_guess_count": 0
        },
        "finding_quality": {
          "finding_precision": 0.027777777777777776,
          "invalid_rate": 0.9722222222222222,
          "hallucination_rate": 0.0,
          "over_flagging_score": 8.75,
          "avg_findings_per_sample": 9.0,
          "total_findings": 36,
          "valid_findings": 1,
          "invalid_findings": 35,
          "hallucinated_findings": 0
        },
        "reasoning_quality": {
          "mean_rcir": 0.75,
          "mean_ava": 0.75,
          "mean_fsv": 1.0,
          "std_rcir": 0.0,
          "std_ava": 0.0,
          "std_fsv": 0.0,
          "n_samples_with_reasoning": 1
        },
        "type_accuracy": {
          "exact_match_rate": 0.0,
          "semantic_match_rate": 1.0,
          "partial_match_rate": 0.0,
          "n_samples": 1
        },
        "calibration": {
          "ece": null,
          "mce": null,
          "overconfidence_rate": null,
          "underconfidence_rate": null,
          "brier_score": null,
          "n_samples": 0
        },
        "composite": {
          "true_understanding_score": 0.00578703703703704,
          "sui": 0.3985294117647059,
          "sui_components": {
            "f2": 0.29411764705882354,
            "target_detection": 0.25,
            "finding_precision": 0.027777777777777776,
            "avg_reasoning": 0.8333333333333334,
            "calibration": 0.5
          },
          "lucky_guess_indicator": 0.0
        }
      },
      "adversarial": {
        "total_samples": 2,
        "vulnerable_samples": 2,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.0,
          "precision": 0,
          "recall": 0.0,
          "f1": 0,
          "f2": 0,
          "fpr": 0,
          "fnr": 1.0,
          "tp": 0,
          "tn": 0,
          "fp": 0,
          "fn": 2
        },
        "target_finding": {
          "target_detection_rate": 0.5,
          "lucky_guess_rate": 0,
          "bonus_discovery_rate": 0.0,
          "target_found_count": 1,
          "lucky_guess_count": 0
        },
        "finding_quality": {
          "finding_precision": 0.16666666666666666,
          "invalid_rate": 0.8333333333333334,
          "hallucination_rate": 0.0,
          "over_flagging_score": 2.5,
          "avg_findings_per_sample": 3.0,
          "total_findings": 6,
          "valid_findings": 1,
          "invalid_findings": 5,
          "hallucinated_findings": 0
        },
        "reasoning_quality": {
          "mean_rcir": 0.75,
          "mean_ava": 0.5,
          "mean_fsv": 0.0,
          "std_rcir": 0.0,
          "std_ava": 0.0,
          "std_fsv": 0.0,
          "n_samples_with_reasoning": 1
        },
        "type_accuracy": {
          "exact_match_rate": 0.0,
          "semantic_match_rate": 0.0,
          "partial_match_rate": 1.0,
          "n_samples": 1
        },
        "calibration": {
          "ece": 1.0,
          "mce": 1.0,
          "overconfidence_rate": 1.0,
          "underconfidence_rate": 0.0,
          "brier_score": 1.0,
          "n_samples": 1
        },
        "composite": {
          "true_understanding_score": 0.03472222222222222,
          "sui": 0.25416666666666665,
          "sui_components": {
            "f2": 0,
            "target_detection": 0.5,
            "finding_precision": 0.16666666666666666,
            "avg_reasoning": 0.4166666666666667,
            "calibration": 0.0
          },
          "lucky_guess_indicator": -0.5
        }
      }
    }
  }
}