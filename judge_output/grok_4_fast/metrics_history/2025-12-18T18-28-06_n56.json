{
  "snapshot_timestamp": "2025-12-18T18:28:06.726059",
  "sample_count": 56,
  "metrics": {
    "total_samples": 56,
    "vulnerable_samples": 56,
    "safe_samples": 0,
    "detection": {
      "accuracy": 0.6785714285714286,
      "precision": 1.0,
      "recall": 0.6785714285714286,
      "f1": 0.8085106382978724,
      "f2": 0.7251908396946566,
      "fpr": 0,
      "fnr": 0.32142857142857145,
      "tp": 38,
      "tn": 0,
      "fp": 0,
      "fn": 18
    },
    "target_finding": {
      "target_detection_rate": 0.3392857142857143,
      "lucky_guess_rate": 0.5789473684210527,
      "bonus_discovery_rate": 0.5178571428571429,
      "target_found_count": 19,
      "lucky_guess_count": 22
    },
    "finding_quality": {
      "finding_precision": 0.43902439024390244,
      "invalid_rate": 0.5609756097560976,
      "hallucination_rate": 0.008130081300813009,
      "over_flagging_score": 1.2321428571428572,
      "avg_findings_per_sample": 2.1964285714285716,
      "total_findings": 123,
      "valid_findings": 54,
      "invalid_findings": 69,
      "hallucinated_findings": 1
    },
    "reasoning_quality": {
      "mean_rcir": 0.9210526315789473,
      "mean_ava": 0.8552631578947368,
      "mean_fsv": 0.8026315789473685,
      "std_rcir": 0.16328517955251126,
      "std_ava": 0.2959795231368945,
      "std_fsv": 0.3679508277397253,
      "n_samples_with_reasoning": 19
    },
    "type_accuracy": {
      "exact_match_rate": 0.6842105263157895,
      "semantic_match_rate": 0.8947368421052632,
      "partial_match_rate": 0.10526315789473684,
      "n_samples": 19
    },
    "calibration": {
      "ece": 0.31160714285714275,
      "mce": 0.3240384615384614,
      "overconfidence_rate": 0.3214285714285714,
      "underconfidence_rate": 0.0,
      "brier_score": 0.3151339285714286,
      "n_samples": 56
    },
    "composite": {
      "true_understanding_score": 0.12804878048780488,
      "sui": 0.6157243634477182,
      "sui_components": {
        "f2": 0.7251908396946566,
        "target_detection": 0.3392857142857143,
        "finding_precision": 0.43902439024390244,
        "avg_reasoning": 0.8596491228070176,
        "calibration": 0.6883928571428573
      },
      "lucky_guess_indicator": 0.3392857142857143
    },
    "by_prompt_type": {
      "direct": {
        "total_samples": 56,
        "vulnerable_samples": 56,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.6785714285714286,
          "precision": 1.0,
          "recall": 0.6785714285714286,
          "f1": 0.8085106382978724,
          "f2": 0.7251908396946566,
          "fpr": 0,
          "fnr": 0.32142857142857145,
          "tp": 38,
          "tn": 0,
          "fp": 0,
          "fn": 18
        },
        "target_finding": {
          "target_detection_rate": 0.3392857142857143,
          "lucky_guess_rate": 0.5789473684210527,
          "bonus_discovery_rate": 0.5178571428571429,
          "target_found_count": 19,
          "lucky_guess_count": 22
        },
        "finding_quality": {
          "finding_precision": 0.43902439024390244,
          "invalid_rate": 0.5609756097560976,
          "hallucination_rate": 0.008130081300813009,
          "over_flagging_score": 1.2321428571428572,
          "avg_findings_per_sample": 2.1964285714285716,
          "total_findings": 123,
          "valid_findings": 54,
          "invalid_findings": 69,
          "hallucinated_findings": 1
        },
        "reasoning_quality": {
          "mean_rcir": 0.9210526315789473,
          "mean_ava": 0.8552631578947368,
          "mean_fsv": 0.8026315789473685,
          "std_rcir": 0.16328517955251126,
          "std_ava": 0.2959795231368945,
          "std_fsv": 0.3679508277397253,
          "n_samples_with_reasoning": 19
        },
        "type_accuracy": {
          "exact_match_rate": 0.6842105263157895,
          "semantic_match_rate": 0.8947368421052632,
          "partial_match_rate": 0.10526315789473684,
          "n_samples": 19
        },
        "calibration": {
          "ece": 0.31160714285714275,
          "mce": 0.3240384615384614,
          "overconfidence_rate": 0.3214285714285714,
          "underconfidence_rate": 0.0,
          "brier_score": 0.3151339285714286,
          "n_samples": 56
        },
        "composite": {
          "true_understanding_score": 0.12804878048780488,
          "sui": 0.6157243634477182,
          "sui_components": {
            "f2": 0.7251908396946566,
            "target_detection": 0.3392857142857143,
            "finding_precision": 0.43902439024390244,
            "avg_reasoning": 0.8596491228070176,
            "calibration": 0.6883928571428573
          },
          "lucky_guess_indicator": 0.3392857142857143
        }
      }
    }
  }
}