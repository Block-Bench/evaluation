# Smart Contract Vulnerability Detection Benchmark
# Default Configuration

data:
  # Path to data directory (relative to project root)
  root: "./raw/data"

  # Ground truth location (annotated metadata)
  ground_truth_path: "./raw/data/annotated/metadata"

  # Transformations to evaluate (one per strategy, nc/no variants only, no sanitized)
  # Pick one representative from each transformation family
  transformations:
    - "nocomments"              # Base: comments stripped
    - "chameleon/gaming_nc"     # Chameleon: gaming theme, no comments
    - "mirror/compressed"       # Mirror: compressed formatting
    - "crossdomain/gaming_no"   # Crossdomain: gaming domain, no comments
    - "hydra/int_nc"            # Hydra: integer variant, no comments
    - "guardianshield/access_control_no"  # GuardianShield: access control, no comments

  # Sampling configuration per subset
  # Set to null or remove to use all samples
  sampling:
    ds: 2   # difficulty_stratified samples
    tc: 2   # temporal_contamination samples
    gs: 2   # gold_standard samples
    strategy: "oneforall"  # Sample base IDs, then get all transformations for each
    min_difficulty: 4  # Only sample contracts with difficulty_tier >= this value

  # Random seed for reproducibility
  seed: 42

evaluation:
  # Prompt types to run (each sample evaluated with each prompt type)
  # Options: direct, naturalistic, adversarial
  prompt_types:
    - "direct"        # Explicit vulnerability analysis request (JSON output)
    - "naturalistic"  # Colleague-style review request (free-form output)
    - "adversarial"   # "Already audited" framing to test sycophancy (free-form output)

  # Whether to request chain-of-thought reasoning (only applies to direct)
  chain_of_thought: false

execution:
  # Maximum concurrent API calls
  max_concurrency: 3

  # Timeout per request in seconds
  timeout_seconds: 180

  # Checkpoint frequency (save progress every N samples)
  checkpoint_every: 10

  # Retry configuration
  max_retries: 3
  retry_delay: 2.0  # Base delay in seconds (exponential backoff)

output:
  # Output directory for results
  directory: "./output"

  # Save raw model responses
  save_raw_responses: true

# Reviewers Configuration
# Select which models to use for evaluation (controls cost)
# Available models (8 total):
#   Vertex AI: claude-opus-4-5, gemini-3-pro, deepseek-v3-2, llama-4-maverick
#   OpenRouter: gpt-5.2, o3, grok-4, qwen3-coder-plus
reviewers:
  # Enable/disable specific reviewers
  enabled:
    # Vertex AI models
    - "claude-opus-4-5"      # Claude Opus 4.5 (global) - Anthropic flagship
    - "gemini-3-pro"         # Gemini 3 Pro Preview (global) - Google flagship
    - "deepseek-v3-2"        # DeepSeek V3.2 (global) - Open-weight leader
    - "llama-4-maverick"     # Llama 4 Maverick (us-east5) - Meta MoE
    # OpenRouter models
    - "gpt-5.2"              # GPT-5.2 - OpenAI flagship
    - "o3"                   # o3 - OpenAI reasoning model
    - "grok-4"               # Grok 4 - xAI flagship
    - "qwen3-coder-plus"     # Qwen3 Coder Plus - Code specialist

  # Run all enabled reviewers in parallel (faster but more costly)
  parallel: false

# Default model to use (can be overridden via CLI)
default_model: "deepseek-v3-2"
