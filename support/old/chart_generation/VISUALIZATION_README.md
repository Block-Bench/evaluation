# Visualization Suite - Setup & Usage

Elegant, publication-quality visualizations of BlockBench evaluation results.

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements_viz.txt
```

### 2. Launch Jupyter Notebook

```bash
jupyter notebook visualization_notebook.ipynb
```

### 3. Run All Cells

- In Jupyter: `Cell > Run All`
- Or run cells individually to explore

---

## Charts Generated

### Chart 1: Model Performance Radar ðŸŽ¯
**File**: `charts/1_radar_performance.html` / `.png`

Multi-dimensional comparison of models across:
- Accuracy
- Target Detection Rate (TDR)
- Finding Precision
- Reasoning Quality
- Calibration

**Insight**: Shows Gemini's balanced performance vs Llama's precision-accuracy gap.

---

### Chart 2: Transformation Impact Heatmap ðŸ”¥
**File**: `charts/2_transformation_heatmap.html` / `.png`

Side-by-side heatmaps showing:
- **Left**: TDR by model Ã— transformation
- **Right**: Finding Precision by model Ã— transformation

**Color Scale**: Red (poor) â†’ Yellow (moderate) â†’ Green (good)

**Insight**: "Sanitized" column is all red - the disaster zone for all models.

---

### Chart 3: TDR vs Lucky Guess Scatter ðŸŽ²
**File**: `charts/3_tdr_vs_lucky_guess.html` / `.png`

Scatter plot exposing the "lucky guess problem":
- **X-axis**: Target Detection Rate (higher = better)
- **Y-axis**: Lucky Guess Rate (lower = better)
- **Bubble size**: Finding Precision (bigger = better)

**Quadrants**:
- Bottom-right (green zone): "True Detector" - finds actual target
- Top-left (red zone): "Lucky Guesser" - detects wrong vulnerability

**Insight**: Llama has 90.9% lucky guess rate; GPT-5.2 has only 37.5% (best).

---

### Chart 4: Sanitization Effect Slope ðŸ“‰
**File**: `charts/4_sanitization_slope.html` / `.png`

Slope chart showing TDR collapse:
- **Left**: Non-sanitized performance (60-85% TDR)
- **Right**: Sanitized performance (5-32% TDR)
- **Lines**: Model performance drop (steeper = worse)

**Insight**: All models suffer 40-60 percentage point drops when code is sanitized.

---

## Color Scheme

Consistent across all charts:

- ðŸŸ£ **Claude Opus 4.5**: Purple (#8B5CF6)
- ðŸŸ¢ **Gemini 3 Pro**: Green (#10B981)
- ðŸ”µ **GPT-5.2**: Blue (#3B82F6)
- ðŸŒ¸ **DeepSeek V3.2**: Pink (#EC4899)
- ðŸŸ  **Llama 3.1 405B**: Orange (#F59E0B)
- ðŸ”® **Grok 4 Fast**: Indigo (#6366F1)

---

## Output Formats

Each chart is saved in two formats:

1. **HTML** (Interactive)
   - Hover for details
   - Zoom/pan support
   - Best for exploration

2. **PNG** (Static, high-res)
   - 2x resolution (Retina quality)
   - Ready for papers/presentations
   - Transparent backgrounds

---

## Customization

### Change Colors

Edit the `MODEL_COLORS` dictionary in cell 2:

```python
MODEL_COLORS = {
    'claude_opus_4.5': '#YOUR_COLOR',
    # ...
}
```

### Export Different Formats

Add to any chart cell:

```python
# PDF export
fig.write_image('charts/chart_name.pdf')

# SVG export (vector graphics)
fig.write_image('charts/chart_name.svg')

# JSON export (data + layout)
fig.write_json('charts/chart_name.json')
```

### Adjust Chart Size

Modify `width` and `height` in `fig.update_layout()`:

```python
fig.update_layout(
    width=1200,   # Adjust width
    height=800,   # Adjust height
)
```

---

## Troubleshooting

### "No module named 'plotly'"

```bash
pip install -r requirements_viz.txt
```

### "Failed to export image"

Install kaleido:

```bash
pip install kaleido
```

### Charts not displaying in notebook

Set renderer:

```python
import plotly.io as pio
pio.renderers.default = 'notebook'
```

### Data files not found

Ensure you're running from the `evaluation/` directory:

```bash
cd /path/to/blockbench/evaluation
jupyter notebook visualization_notebook.ipynb
```

---

## Data Sources

Charts use these data files (auto-generated by evaluation pipeline):

- `TRANSFORMATION_ANALYSIS.json` - Performance by transformation
- `GS_PERFORMANCE_ANALYSIS.json` - GPTShield dataset performance
- `judge_output/{model}/aggregated_metrics.json` - Per-model metrics

---

## Advanced Usage

### Regenerate Data

If you've re-run evaluations, regenerate the analysis data:

```bash
# Regenerate transformation analysis
python scripts/analyze_by_transformation.py

# Regenerate GS performance analysis
python scripts/analyze_gs_performance.py
```

### Add New Charts

Copy any chart cell and modify:
1. Change data source
2. Update chart type
3. Adjust styling
4. Save with new filename

---

## Citation

When using these visualizations in publications, please cite:

```bibtex
@misc{blockbench2025,
  title={BlockBench: Evaluating LLMs for Smart Contract Vulnerability Detection},
  author={Your Name},
  year={2025},
  note={Dataset: TC (temporal contamination), GS (GPTShield), DS (DiverseVul)}
}
```

---

## Need Help?

- Check the notebook comments for detailed explanations
- Review the analysis markdown files:
  - `TRANSFORMATION_IMPACT_ANALYSIS.md`
  - `GS_PERFORMANCE_SUMMARY.md`
  - `PROMPT_TYPE_RESULTS.md`

---

**Happy Visualizing! ðŸŽ¨**
