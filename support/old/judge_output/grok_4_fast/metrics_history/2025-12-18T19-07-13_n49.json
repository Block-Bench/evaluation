{
  "snapshot_timestamp": "2025-12-18T19:07:13.920954",
  "sample_count": 49,
  "metrics": {
    "total_samples": 49,
    "vulnerable_samples": 49,
    "safe_samples": 0,
    "detection": {
      "accuracy": 0.6122448979591837,
      "precision": 1.0,
      "recall": 0.6122448979591837,
      "f1": 0.759493670886076,
      "f2": 0.6637168141592921,
      "fpr": 0,
      "fnr": 0.3877551020408163,
      "tp": 30,
      "tn": 0,
      "fp": 0,
      "fn": 19
    },
    "target_finding": {
      "target_detection_rate": 0.32653061224489793,
      "lucky_guess_rate": 0.5333333333333333,
      "bonus_discovery_rate": 0.4897959183673469,
      "target_found_count": 16,
      "lucky_guess_count": 16
    },
    "finding_quality": {
      "finding_precision": 0.36507936507936506,
      "invalid_rate": 0.6349206349206349,
      "hallucination_rate": 0.015873015873015872,
      "over_flagging_score": 1.6326530612244898,
      "avg_findings_per_sample": 2.5714285714285716,
      "total_findings": 126,
      "valid_findings": 46,
      "invalid_findings": 80,
      "hallucinated_findings": 2
    },
    "reasoning_quality": {
      "mean_rcir": 0.9375,
      "mean_ava": 0.90625,
      "mean_fsv": 0.84375,
      "std_rcir": 0.13975424859373686,
      "std_ava": 0.21423920626253262,
      "std_fsv": 0.27775607554111215,
      "n_samples_with_reasoning": 16
    },
    "type_accuracy": {
      "exact_match_rate": 0.625,
      "semantic_match_rate": 0.875,
      "partial_match_rate": 0.125,
      "n_samples": 16
    },
    "calibration": {
      "ece": 0.3687500000000001,
      "mce": 0.4,
      "overconfidence_rate": 0.375,
      "underconfidence_rate": 0.0,
      "brier_score": 0.3671875,
      "n_samples": 48
    },
    "composite": {
      "true_understanding_score": 0.10679192311845374,
      "sui": 0.5894070946962856,
      "sui_components": {
        "f2": 0.6637168141592921,
        "target_detection": 0.32653061224489793,
        "finding_precision": 0.36507936507936506,
        "avg_reasoning": 0.8958333333333334,
        "calibration": 0.6312499999999999
      },
      "lucky_guess_indicator": 0.28571428571428575
    },
    "by_prompt_type": {
      "direct": {
        "total_samples": 46,
        "vulnerable_samples": 46,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.6521739130434783,
          "precision": 1.0,
          "recall": 0.6521739130434783,
          "f1": 0.7894736842105263,
          "f2": 0.7009345794392524,
          "fpr": 0,
          "fnr": 0.34782608695652173,
          "tp": 30,
          "tn": 0,
          "fp": 0,
          "fn": 16
        },
        "target_finding": {
          "target_detection_rate": 0.32608695652173914,
          "lucky_guess_rate": 0.5333333333333333,
          "bonus_discovery_rate": 0.5217391304347826,
          "target_found_count": 15,
          "lucky_guess_count": 16
        },
        "finding_quality": {
          "finding_precision": 0.4205607476635514,
          "invalid_rate": 0.5794392523364486,
          "hallucination_rate": 0.018691588785046728,
          "over_flagging_score": 1.3478260869565217,
          "avg_findings_per_sample": 2.3260869565217392,
          "total_findings": 107,
          "valid_findings": 45,
          "invalid_findings": 62,
          "hallucinated_findings": 2
        },
        "reasoning_quality": {
          "mean_rcir": 0.95,
          "mean_ava": 0.9166666666666666,
          "mean_fsv": 0.8666666666666667,
          "std_rcir": 0.135400640077266,
          "std_ava": 0.21730674684008827,
          "std_fsv": 0.27182510717166813,
          "n_samples_with_reasoning": 15
        },
        "type_accuracy": {
          "exact_match_rate": 0.6666666666666666,
          "semantic_match_rate": 0.8666666666666667,
          "partial_match_rate": 0.13333333333333333,
          "n_samples": 15
        },
        "calibration": {
          "ece": 0.341304347826087,
          "mce": 0.4,
          "overconfidence_rate": 0.34782608695652173,
          "underconfidence_rate": 0.0,
          "brier_score": 0.33967391304347827,
          "n_samples": 46
        },
        "composite": {
          "true_understanding_score": 0.12494920763917108,
          "sui": 0.6134868391349497,
          "sui_components": {
            "f2": 0.7009345794392524,
            "target_detection": 0.32608695652173914,
            "finding_precision": 0.4205607476635514,
            "avg_reasoning": 0.9111111111111111,
            "calibration": 0.658695652173913
          },
          "lucky_guess_indicator": 0.32608695652173914
        }
      },
      "adversarial": {
        "total_samples": 2,
        "vulnerable_samples": 2,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.0,
          "precision": 0,
          "recall": 0.0,
          "f1": 0,
          "f2": 0,
          "fpr": 0,
          "fnr": 1.0,
          "tp": 0,
          "tn": 0,
          "fp": 0,
          "fn": 2
        },
        "target_finding": {
          "target_detection_rate": 0.0,
          "lucky_guess_rate": 0,
          "bonus_discovery_rate": 0.0,
          "target_found_count": 0,
          "lucky_guess_count": 0
        },
        "finding_quality": {
          "finding_precision": 0.0,
          "invalid_rate": 1.0,
          "hallucination_rate": 0.0,
          "over_flagging_score": 6.0,
          "avg_findings_per_sample": 6.0,
          "total_findings": 12,
          "valid_findings": 0,
          "invalid_findings": 12,
          "hallucinated_findings": 0
        },
        "reasoning_quality": {
          "mean_rcir": null,
          "mean_ava": null,
          "mean_fsv": null,
          "std_rcir": null,
          "std_ava": null,
          "std_fsv": null,
          "n_samples_with_reasoning": 0
        },
        "type_accuracy": {
          "exact_match_rate": 0,
          "semantic_match_rate": 0,
          "partial_match_rate": 0,
          "n_samples": 0
        },
        "calibration": {
          "ece": 1.0,
          "mce": 1.0,
          "overconfidence_rate": 1.0,
          "underconfidence_rate": 0.0,
          "brier_score": 1.0,
          "n_samples": 2
        },
        "composite": {
          "true_understanding_score": 0.0,
          "sui": 0.0,
          "sui_components": {
            "f2": 0,
            "target_detection": 0.0,
            "finding_precision": 0.0,
            "avg_reasoning": 0,
            "calibration": 0.0
          },
          "lucky_guess_indicator": 0.0
        }
      },
      "naturalistic": {
        "total_samples": 1,
        "vulnerable_samples": 1,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.0,
          "precision": 0,
          "recall": 0.0,
          "f1": 0,
          "f2": 0,
          "fpr": 0,
          "fnr": 1.0,
          "tp": 0,
          "tn": 0,
          "fp": 0,
          "fn": 1
        },
        "target_finding": {
          "target_detection_rate": 1.0,
          "lucky_guess_rate": 0,
          "bonus_discovery_rate": 0.0,
          "target_found_count": 1,
          "lucky_guess_count": 0
        },
        "finding_quality": {
          "finding_precision": 0.14285714285714285,
          "invalid_rate": 0.8571428571428571,
          "hallucination_rate": 0.0,
          "over_flagging_score": 6.0,
          "avg_findings_per_sample": 7.0,
          "total_findings": 7,
          "valid_findings": 1,
          "invalid_findings": 6,
          "hallucinated_findings": 0
        },
        "reasoning_quality": {
          "mean_rcir": 0.75,
          "mean_ava": 0.75,
          "mean_fsv": 0.5,
          "std_rcir": 0.0,
          "std_ava": 0.0,
          "std_fsv": 0.0,
          "n_samples_with_reasoning": 1
        },
        "type_accuracy": {
          "exact_match_rate": 0.0,
          "semantic_match_rate": 1.0,
          "partial_match_rate": 0.0,
          "n_samples": 1
        },
        "calibration": {
          "ece": null,
          "mce": null,
          "overconfidence_rate": null,
          "underconfidence_rate": null,
          "brier_score": null,
          "n_samples": 0
        },
        "composite": {
          "true_understanding_score": 0.09523809523809526,
          "sui": 0.4880952380952381,
          "sui_components": {
            "f2": 0,
            "target_detection": 1.0,
            "finding_precision": 0.14285714285714285,
            "avg_reasoning": 0.6666666666666666,
            "calibration": 0.5
          },
          "lucky_guess_indicator": -1.0
        }
      }
    }
  }
}