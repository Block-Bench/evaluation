{
  "snapshot_timestamp": "2025-12-18T19:04:57.310920",
  "sample_count": 46,
  "metrics": {
    "total_samples": 46,
    "vulnerable_samples": 46,
    "safe_samples": 0,
    "detection": {
      "accuracy": 0.7391304347826086,
      "precision": 1.0,
      "recall": 0.7391304347826086,
      "f1": 0.85,
      "f2": 0.7798165137614679,
      "fpr": 0,
      "fnr": 0.2608695652173913,
      "tp": 34,
      "tn": 0,
      "fp": 0,
      "fn": 12
    },
    "target_finding": {
      "target_detection_rate": 0.5434782608695652,
      "lucky_guess_rate": 0.29411764705882354,
      "bonus_discovery_rate": 0.6521739130434783,
      "target_found_count": 25,
      "lucky_guess_count": 10
    },
    "finding_quality": {
      "finding_precision": 0.6065573770491803,
      "invalid_rate": 0.39344262295081966,
      "hallucination_rate": 0.00819672131147541,
      "over_flagging_score": 1.0434782608695652,
      "avg_findings_per_sample": 2.652173913043478,
      "total_findings": 122,
      "valid_findings": 74,
      "invalid_findings": 48,
      "hallucinated_findings": 1
    },
    "reasoning_quality": {
      "mean_rcir": 0.96,
      "mean_ava": 0.97,
      "mean_fsv": 0.95,
      "std_rcir": 0.09165151389911681,
      "std_ava": 0.1077032961426901,
      "std_fsv": 0.1414213562373095,
      "n_samples_with_reasoning": 25
    },
    "type_accuracy": {
      "exact_match_rate": 0.6,
      "semantic_match_rate": 0.92,
      "partial_match_rate": 0.08,
      "n_samples": 25
    },
    "calibration": {
      "ece": 0.0602564102564103,
      "mce": 0.62,
      "overconfidence_rate": 0.06896551724137934,
      "underconfidence_rate": 0.0,
      "brier_score": 0.10871025641025642,
      "n_samples": 39
    },
    "composite": {
      "true_understanding_score": 0.3164647184604419,
      "sui": 0.7557816591894942,
      "sui_components": {
        "f2": 0.7798165137614679,
        "target_detection": 0.5434782608695652,
        "finding_precision": 0.6065573770491803,
        "avg_reasoning": 0.96,
        "calibration": 0.9397435897435897
      },
      "lucky_guess_indicator": 0.19565217391304346
    },
    "by_prompt_type": {
      "direct": {
        "total_samples": 39,
        "vulnerable_samples": 39,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.8461538461538461,
          "precision": 1.0,
          "recall": 0.8461538461538461,
          "f1": 0.9166666666666666,
          "f2": 0.8730158730158731,
          "fpr": 0,
          "fnr": 0.15384615384615385,
          "tp": 33,
          "tn": 0,
          "fp": 0,
          "fn": 6
        },
        "target_finding": {
          "target_detection_rate": 0.5897435897435898,
          "lucky_guess_rate": 0.30303030303030304,
          "bonus_discovery_rate": 0.6410256410256411,
          "target_found_count": 23,
          "lucky_guess_count": 10
        },
        "finding_quality": {
          "finding_precision": 0.8611111111111112,
          "invalid_rate": 0.1388888888888889,
          "hallucination_rate": 0.013888888888888888,
          "over_flagging_score": 0.2564102564102564,
          "avg_findings_per_sample": 1.8461538461538463,
          "total_findings": 72,
          "valid_findings": 62,
          "invalid_findings": 10,
          "hallucinated_findings": 1
        },
        "reasoning_quality": {
          "mean_rcir": 0.9782608695652174,
          "mean_ava": 1.0,
          "mean_fsv": 0.9891304347826086,
          "std_rcir": 0.07044283367834632,
          "std_ava": 0.0,
          "std_fsv": 0.050982779998080756,
          "n_samples_with_reasoning": 23
        },
        "type_accuracy": {
          "exact_match_rate": 0.6521739130434783,
          "semantic_match_rate": 0.9130434782608695,
          "partial_match_rate": 0.08695652173913043,
          "n_samples": 23
        },
        "calibration": {
          "ece": 0.0602564102564103,
          "mce": 0.62,
          "overconfidence_rate": 0.06896551724137934,
          "underconfidence_rate": 0.0,
          "brier_score": 0.10871025641025642,
          "n_samples": 39
        },
        "composite": {
          "true_understanding_score": 0.5023148148148149,
          "sui": 0.8361135000265436,
          "sui_components": {
            "f2": 0.8730158730158731,
            "target_detection": 0.5897435897435898,
            "finding_precision": 0.8611111111111112,
            "avg_reasoning": 0.9891304347826088,
            "calibration": 0.9397435897435897
          },
          "lucky_guess_indicator": 0.2564102564102564
        }
      },
      "naturalistic": {
        "total_samples": 4,
        "vulnerable_samples": 4,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.25,
          "precision": 1.0,
          "recall": 0.25,
          "f1": 0.4,
          "f2": 0.29411764705882354,
          "fpr": 0,
          "fnr": 0.75,
          "tp": 1,
          "tn": 0,
          "fp": 0,
          "fn": 3
        },
        "target_finding": {
          "target_detection_rate": 0.25,
          "lucky_guess_rate": 0.0,
          "bonus_discovery_rate": 1.0,
          "target_found_count": 1,
          "lucky_guess_count": 0
        },
        "finding_quality": {
          "finding_precision": 0.3448275862068966,
          "invalid_rate": 0.6551724137931034,
          "hallucination_rate": 0.0,
          "over_flagging_score": 4.75,
          "avg_findings_per_sample": 7.25,
          "total_findings": 29,
          "valid_findings": 10,
          "invalid_findings": 19,
          "hallucinated_findings": 0
        },
        "reasoning_quality": {
          "mean_rcir": 0.75,
          "mean_ava": 0.5,
          "mean_fsv": 0.5,
          "std_rcir": 0.0,
          "std_ava": 0.0,
          "std_fsv": 0.0,
          "n_samples_with_reasoning": 1
        },
        "type_accuracy": {
          "exact_match_rate": 0.0,
          "semantic_match_rate": 1.0,
          "partial_match_rate": 0.0,
          "n_samples": 1
        },
        "calibration": {
          "ece": null,
          "mce": null,
          "overconfidence_rate": null,
          "underconfidence_rate": null,
          "brier_score": null,
          "n_samples": 0
        },
        "composite": {
          "true_understanding_score": 0.05028735632183909,
          "sui": 0.38358688302907373,
          "sui_components": {
            "f2": 0.29411764705882354,
            "target_detection": 0.25,
            "finding_precision": 0.3448275862068966,
            "avg_reasoning": 0.5833333333333334,
            "calibration": 0.5
          },
          "lucky_guess_indicator": 0.0
        }
      },
      "adversarial": {
        "total_samples": 3,
        "vulnerable_samples": 3,
        "safe_samples": 0,
        "detection": {
          "accuracy": 0.0,
          "precision": 0,
          "recall": 0.0,
          "f1": 0,
          "f2": 0,
          "fpr": 0,
          "fnr": 1.0,
          "tp": 0,
          "tn": 0,
          "fp": 0,
          "fn": 3
        },
        "target_finding": {
          "target_detection_rate": 0.3333333333333333,
          "lucky_guess_rate": 0,
          "bonus_discovery_rate": 0.3333333333333333,
          "target_found_count": 1,
          "lucky_guess_count": 0
        },
        "finding_quality": {
          "finding_precision": 0.09523809523809523,
          "invalid_rate": 0.9047619047619048,
          "hallucination_rate": 0.0,
          "over_flagging_score": 6.333333333333333,
          "avg_findings_per_sample": 7.0,
          "total_findings": 21,
          "valid_findings": 2,
          "invalid_findings": 19,
          "hallucinated_findings": 0
        },
        "reasoning_quality": {
          "mean_rcir": 0.75,
          "mean_ava": 0.75,
          "mean_fsv": 0.5,
          "std_rcir": 0.0,
          "std_ava": 0.0,
          "std_fsv": 0.0,
          "n_samples_with_reasoning": 1
        },
        "type_accuracy": {
          "exact_match_rate": 0.0,
          "semantic_match_rate": 1.0,
          "partial_match_rate": 0.0,
          "n_samples": 1
        },
        "calibration": {
          "ece": null,
          "mce": null,
          "overconfidence_rate": null,
          "underconfidence_rate": null,
          "brier_score": null,
          "n_samples": 0
        },
        "composite": {
          "true_understanding_score": 0.021164021164021163,
          "sui": 0.3142857142857143,
          "sui_components": {
            "f2": 0,
            "target_detection": 0.3333333333333333,
            "finding_precision": 0.09523809523809523,
            "avg_reasoning": 0.6666666666666666,
            "calibration": 0.5
          },
          "lucky_guess_indicator": -0.3333333333333333
        }
      }
    }
  }
}