\section{Methodology}

Our evaluation methodology comprises four phases: adversarial transformation, model evaluation, automated judgment, and metrics computation. Figure~\ref{fig:pipeline} illustrates the complete pipeline.

\begin{figure}[h]
\centering
\small
\begin{tikzpicture}[node distance=0.3cm and 0.2cm,
    phase/.style={rectangle, draw, rounded corners, minimum width=1.2cm, minimum height=0.6cm, align=center, font=\tiny},
    arrow/.style={->, thick}]

\node[phase, fill=blue!15] (data) {Data};
\node[phase, fill=blue!15, right=of data] (transform) {Transform};
\node[phase, fill=orange!15, right=of transform] (prompt) {Prompt/\\Detect};
\node[phase, fill=purple!15, right=of prompt] (judge) {Judge\\(LLM+\\Human)};
\node[phase, fill=green!15, right=of judge] (metrics) {Metrics};

\draw[arrow] (data) -- (transform);
\draw[arrow] (transform) -- (prompt);
\draw[arrow] (prompt) -- (judge);
\draw[arrow] (judge) -- (metrics);
\end{tikzpicture}
\caption{BlockBench evaluation pipeline.}
\label{fig:pipeline}
\end{figure}

\subsection{Adversarial Transformations}

To distinguish memorization from understanding, we apply semantic-preserving transformations that systematically remove surface cues while preserving vulnerability semantics. For each contract $c \in \mathcal{D}$, we generate variants $\{\mathcal{T}_k(c)\}$ satisfying $\mathcal{V}(\mathcal{T}(c)) = \mathcal{V}(c)$, where $\mathcal{V}$ extracts vulnerability semantics.

\textbf{Sanitization (sn)} removes security hints from identifiers and comments through 280+ pattern replacements while maintaining natural code style. \textbf{No-Comments (nc)} strips all documentation. \textbf{Chameleon (ch)} replaces blockchain terminology with domain-shifted vocabulary (medical, gaming themes). \textbf{Shapeshifter (ss)} applies multi-level obfuscation from identifier renaming (L2) to control flow obscuration (L3). This pipeline generates 1,343 variants from 263 base samples. Complete transformation specifications appear in Appendix~\ref{app:transformations}.

\subsection{Evaluation Protocol}

We evaluate six frontier models (Claude Opus 4.5, GPT-5.2, Gemini 3 Pro, Grok 4, DeepSeek v3.2, Llama 3.1 405B) using three prompt types. \textit{Direct} requests structured JSON analysis. \textit{Naturalistic} provides informal review requests. \textit{Adversarial} includes misleading context claiming prior audit approval. All models use consistent parameters (temperature 0, max tokens 8192). Prompt templates appear in Appendix~\ref{app:prompts}.

\subsection{Automated Judgment}

Mistral Medium 3 serves as LLM judge, evaluating responses against ground truth. The judge classifies findings as TARGET\_MATCH, BONUS\_VALID, or invalid (HALLUCINATED, MISCHARACTERIZED, SECURITY\_THEATER). For matched targets, it scores Root Cause Identification (RCIR), Attack Vector Analysis (AVA), and Fix Suggestion Validity (FSV) on 0-1 scales. Human validation of 31 samples (116 comparisons across models) confirms reliability ($\kappa$=0.84, $\rho$=0.85, F1=0.91). Complete judge protocol appears in Appendix~\ref{app:judge}.

\subsection{Metrics}

We rank models by \textit{Target Detection Rate} (TDR), the proportion of samples where the documented vulnerability was correctly identified with both type and location accuracy. \textit{Lucky Guess Rate} measures correct verdicts without target identification. \textit{Finding Precision} computes the proportion of reported findings that are correct. \textit{Reasoning Quality} averages RCIR, AVA, and FSV scores for successfully identified targets.

We report \textit{Security Understanding Index} (SUI) as a weighted composite: SUI = 0.40·TDR + 0.30·Reasoning + 0.30·Precision. Sensitivity analysis across five weight configurations confirms perfect ranking stability (Spearman's $\rho$=1.000). Complete metric definitions and sensitivity analysis appear in Appendix~\ref{app:metrics} and ~\ref{app:sensitivity}.
