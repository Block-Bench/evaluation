\section{Methodology}

\textbf{Adversarial Transformations.}
To distinguish memorization from understanding, we apply semantic-preserving transformations that remove surface cues while preserving vulnerability semantics. \textit{Sanitization} removes security hints from identifiers and comments. \textit{No-Comments} strips all documentation. \textit{Chameleon} replaces blockchain terminology with domain-shifted vocabulary (medical, gaming). \textit{Shapeshifter} applies multi-level obfuscation from simple renaming (L2) to control flow obscuration (L3). These transformations generate 1,343 variants from 263 base samples. Full transformation specifications appear in Appendix~\ref{app:transformations}.

\textbf{Evaluation Protocol.}
We evaluate models using three prompt types. \textit{Direct} requests structured JSON analysis measuring technical capability. \textit{Naturalistic} provides informal review requests testing natural reasoning. \textit{Adversarial} includes suggestive framing (``Our senior auditor approved this'') measuring resistance to authority bias. See Appendix~\ref{app:prompts} for templates.

\textbf{Automated Judgment.}
Mistral Medium 3 serves as LLM judge, evaluating each response against ground truth through multi-stage analysis. The judge classifies findings as TARGET\_MATCH, BONUS\_VALID (genuine undocumented vulnerabilities), or invalid categories (HALLUCINATED, MISCHARACTERIZED, SECURITY\_THEATER). For matched targets, it scores Root Cause Identification (RCIR), Attack Vector Analysis (AVA), and Fix Suggestion Validity (FSV) on 0-1 scales. Human evaluation of 20 responses validates judge reliability (κ=0.91 verdict agreement, ρ=0.87 correlation).

\textbf{Metrics.}
We rank models by \textit{Target Detection Rate} (TDR), the proportion of samples where the specific documented vulnerability was correctly identified, requiring both type and location accuracy. Supporting metrics include \textit{Lucky Guess Rate} (correct verdicts without target identification), \textit{Finding Precision} (proportion of valid findings), and \textit{Reasoning Quality} (mean of RCIR, AVA, FSV). We report \textit{Security Understanding Index} (SUI), a composite weighting TDR (40\%), reasoning (30\%), and precision (30\%). Sensitivity analysis across five weight configurations confirms ranking stability (ρ=0.949, Appendix~\ref{app:sensitivity}). Complete metric definitions appear in Appendix~\ref{app:metrics}.
