\section{BlockBench}

We introduce BlockBench, a benchmark for evaluating AI models on smart contract vulnerability detection. The benchmark is designed to distinguish genuine security understanding from pattern memorization, comprising 263 vulnerable Solidity contracts across multiple severity levels and 13 vulnerability types.

Let $\mathcal{D}$ represent the dataset, where $\mathcal{D} = \{(c_i, v_i, m_i)\}_{i=1}^{263}$. Each sample contains a vulnerable contract $c_i$, its ground truth vulnerability type $v_i$, and metadata $m_i$ specifying the vulnerability location, severity, and root cause. We partition $\mathcal{D}$ into three disjoint subsets, $\mathcal{D} = \mathcal{D}_{\text{DS}} \cup \mathcal{D}_{\text{TC}} \cup \mathcal{D}_{\text{GS}}$, each targeting a distinct evaluation objective (Table~\ref{tab:dataset}).

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lrp{2.8cm}@{}}
\toprule
\textbf{Subset} & \textbf{N} & \textbf{Sources} \\
\midrule
Difficulty Stratified & 179 & SmartBugs, ToB \\
Temporal Contam. & 50 & DeFiHackLabs \\
Gold Standard & 34 & Spearbit, C4 \\
\bottomrule
\end{tabular}
\caption{BlockBench composition spanning Critical, High, Medium, and Low severity.}
\label{tab:dataset}
\end{table}

\paragraph{Difficulty Stratified.} $\mathcal{D}_{\text{DS}}$ draws from established vulnerability repositories including SmartBugs Curated \citep{ferreira2020smartbugs}, Trail of Bits' Not So Smart Contracts \citep{trailofbits2018}, and DeFiVulnLabs \citep{defivulnlabs2023}. Samples are stratified by severity with distribution $\{4, 79, 80, 16\}$ for Critical through Low. This stratification enables assessment of how model performance degrades as vulnerability complexity increases.

\paragraph{Temporal Contamination.} $\mathcal{D}_{\text{TC}}$ reconstructs well-known exploits from DeFiHackLabs \citep{defihacklabs2024} and the REKT Database \citep{rekt2023}, including Nomad Bridge (\$190M), Beanstalk (\$182M), and Curve Vyper (\$70M). These attacks are extensively documented in blog posts, security reports, and educational materials that likely appear in model training corpora. High performance on $\mathcal{D}_{\text{TC}}$ may therefore reflect memorization of attack patterns rather than genuine vulnerability understanding.

\paragraph{Gold Standard.} $\mathcal{D}_{\text{GS}}$ derives from professional security audits by Spearbit \citep{spearbit2025}, MixBytes \citep{mixbytes2025}, and Code4rena \citep{code4rena2025} conducted after September 2025. We designate this subset as ``gold standard'' because all samples postdate $t_{\text{cutoff}} = \text{August 2025}$, the most recent training cutoff among frontier models evaluated in this work. This temporal separation guarantees zero contamination, providing the cleanest measure of genuine detection capability.

\paragraph{Coverage.} BlockBench spans 13 vulnerability classes. Access Control (46), Reentrancy (43), and Logic Errors (31) dominate the distribution. $\mathcal{D}_{\text{TC}}$ emphasizes oracle manipulation and access control. $\mathcal{D}_{\text{GS}}$ focuses on subtle logic errors. $\mathcal{D}_{\text{DS}}$ provides broad coverage across classical patterns.
