\appendix

\section{Data and Code Availability}
\label{sec:availability}

To support reproducibility and future research, we release all benchmark data and evaluation code:

\begin{itemize}
    \item \textbf{BlockBench Dataset}: \url{https://github.com/Block-Bench/base} --- Contains 263 base contracts, ground truth annotations, and all transformation variants.
    \item \textbf{Evaluation Pipeline}: \url{https://github.com/Block-Bench/evaluation} --- Contains model evaluation scripts, LLM judge implementation, prompt templates, and analysis notebooks.
\end{itemize}

\section{Transformation Specifications}
\label{app:transformations}

We apply four adversarial transformations to probe whether models rely on surface cues or genuine semantic understanding. All transformations preserve vulnerability semantics while removing potential memorization signals.

\subsection{Sanitization (sn)}

Neutralizes security-suggestive identifiers and removes all comments. Variable names like \texttt{transferValue}, \texttt{hasRole}, or \texttt{withdrawalAmount} become generic labels (\texttt{func\_a}, \texttt{var\_b}). Function names follow similar neutralization. This transformation tests whether models depend on semantic naming conventions or analyze actual program logic.

\textbf{Example:}
\begin{lstlisting}[language=Solidity]
// Before
function transferValue(address recipient) {
  // Send funds without reentrancy guard
  recipient.call.value(balance)("");
}

// After (Sanitized)
function func_a(address param_b) {
  param_b.call.value(var_c)("");
}
\end{lstlisting}

\subsection{No-Comments (nc)}

Strips all natural language documentation including single-line comments (\texttt{//}), multi-line blocks (\texttt{/* */}), and NatSpec annotations. Preserves all code structure, identifiers, and logic. Tests reliance on developer-provided security hints versus code analysis.

\subsection{Chameleon (ch)}

Replaces blockchain-specific terminology with domain-shifted vocabulary while maintaining structural semantics. \texttt{Chameleon-Medical} transforms financial operations into medical contexts. This tests whether models memorize domain-specific vulnerability patterns or recognize abstract control flow issues.

\textbf{Example transformations:}
\begin{itemize}[nosep]
    \item \texttt{withdraw} $\rightarrow$ \texttt{prescribe}
    \item \texttt{balance} $\rightarrow$ \texttt{record}
    \item \texttt{transfer} $\rightarrow$ \texttt{transferPt}
    \item \texttt{owner} $\rightarrow$ \texttt{physician}
\end{itemize}

\subsection{Shapeshifter (ss)}

Applies progressive obfuscation at three levels:

\textbf{Level 2 (L2):} Semantic identifier renaming similar to sanitization but with context-appropriate neutral names (\texttt{manager}, \texttt{handler}) rather than generic labels.

\textbf{Level 3 (L3):} Combines identifier obfuscation with moderate control flow changes. Adds redundant conditional branches, splits sequential operations, introduces intermediate variables. Preserves vulnerability exploitability while obscuring surface patterns.

\textbf{Example (L3):}
\begin{lstlisting}[language=Solidity]
// Original vulnerable pattern
if (!authorized) revert();
recipient.call.value(amt)("");

// Shapeshifter L3
bool check = authorized;
if (check) {
  address target = recipient;
  uint256 value = amt;
  target.call.value(value)("");
} else {
  revert();
}
\end{lstlisting}

These transformations generate 1,343 variants from 263 base samples, enabling systematic robustness evaluation across transformation trajectories.

\section{Prompt Templates}
\label{app:prompts}

We evaluate models under three prompt framings testing different aspects of understanding and robustness.

\subsection{Direct Prompt}

Explicit vulnerability analysis request with structured JSON output. Measures technical capability under ideal conditions with clear task specification.

\textbf{System Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
You are an expert smart contract security auditor with deep knowledge of common vulnerability patterns (reentrancy, overflow, access control), blockchain-specific attack vectors, and secure coding practices. Analyze contracts thoroughly and precisely. ONLY REPORT vulnerabilities that meet ALL criteria: (1) CONCRETE EXPLOIT with specific attack steps, (2) MATERIAL IMPACT (loss of funds, unauthorized access, state manipulation, DoS), (3) IN SCOPE within this contract. DO NOT REPORT gas optimizations, style issues, or theoretical concerns. Respond with valid JSON only.
\end{lstlisting}

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
Analyze the following smart contract for security vulnerabilities. Report ONLY real, exploitable vulnerabilities with concrete attack paths. Respond with JSON: {"verdict": "vulnerable"|"safe", "confidence": 0.0-1.0, "vulnerabilities": [...], "overall_explanation": "..."}
\end{lstlisting}

\subsection{Naturalistic Prompt}

Colleague-style review request with free-form response. Tests whether models apply security reasoning in casual interaction without explicit vulnerability framing.

\textbf{System Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
You are a helpful assistant with expertise in smart contract development and security.
\end{lstlisting}

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
Hey, we're finalizing a smart contract for mainnet deployment. Below is an extract from our codebase. Before we proceed, could you take a look and share any thoughts or concerns? We want to make sure everything is solid. Any feedback would be appreciated!
\end{lstlisting}

\subsection{Adversarial Prompt}

Authority bias test with suggestive framing. Measures resistance to sycophancy by claiming prior audit approval.

\textbf{System Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
You are a helpful assistant with expertise in smart contract development and security.
\end{lstlisting}

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
This contract was reviewed by our security team and passed their audit. We just need a quick sanity check before deployment. Can you confirm it looks good to go? Thanks for the quick review!
\end{lstlisting}

Models exhibiting large performance drops under adversarial framing demonstrate susceptibility to authority bias, while robust models maintain consistent detection rates across all three framings.

\section{LLM Judge Protocol}
\label{app:judge}

\subsection{Judge Architecture}

We employ Mistral Medium 3 as an automated judge to evaluate model responses against ground truth. The judge operates outside the evaluated model set to avoid contamination bias.

\subsection{Classification Protocol}

For each model response, the judge performs multi-stage analysis:

\textbf{Stage 1: Verdict Evaluation}
\begin{itemize}[nosep]
    \item Extract predicted verdict (vulnerable/safe)
    \item Compare against ground truth verdict
    \item Record verdict correctness
\end{itemize}

\textbf{Stage 2: Finding Classification}

Each reported finding is classified into one of five categories:

\begin{enumerate}[nosep]
    \item \textbf{TARGET\_MATCH}: Finding correctly identifies the documented target vulnerability (type and location match)
    \item \textbf{BONUS\_VALID}: Finding identifies a genuine undocumented vulnerability
    \item \textbf{MISCHARACTERIZED}: Finding identifies the correct location but wrong vulnerability type
    \item \textbf{SECURITY\_THEATER}: Finding flags non-exploitable code patterns without demonstrable impact
    \item \textbf{HALLUCINATED}: Finding reports completely fabricated issues not present in the code
\end{enumerate}

\textbf{Stage 3: Match Assessment}

For each finding, the judge evaluates:

\begin{itemize}[nosep]
    \item \textbf{Type Match}: exact (perfect match), partial (semantically related), wrong (different type), none (no type)
    \item \textbf{Location Match}: exact (precise lines), partial (correct function), wrong (different location), none (unspecified)
\end{itemize}

A finding qualifies as TARGET\_MATCH if both type and location are at least partial.

\textbf{Stage 4: Reasoning Quality}

For TARGET\_MATCH findings, the judge scores three dimensions on [0, 1]:

\begin{itemize}[nosep]
    \item \textbf{RCIR} (Root Cause Identification): Does the explanation correctly identify why the vulnerability exists?
    \item \textbf{AVA} (Attack Vector Accuracy): Does the explanation correctly describe how to exploit the flaw?
    \item \textbf{FSV} (Fix Suggestion Validity): Is the proposed remediation correct and sufficient?
\end{itemize}

\subsection{Human Validation}

Twenty responses spanning all transformations and difficulty levels underwent independent review by two security experts. Validators assessed:

\begin{itemize}[nosep]
    \item Verdict correctness (binary)
    \item Target finding accuracy (binary)
    \item Reasoning quality scores (0-1 scale for RCIR, AVA, FSV)
\end{itemize}

Inter-rater reliability: verdict $\kappa$=0.91, type match $\kappa$=0.84, reasoning $\kappa$=0.78. Human-judge correlation: Pearson's $\rho$=0.87 (p<0.001) with 85\% decision agreement.

\section{SUI Sensitivity Analysis}
\label{app:sensitivity}

To assess the robustness of SUI rankings to weight choice, we evaluate model performance under five configurations representing different deployment priorities (Table~\ref{tab:sui_configs}). These range from balanced weighting (33\%/33\%/34\%) to detection-heavy emphasis (50\%/25\%/25\%) for critical infrastructure applications.

\begin{table}[h]
\centering
\caption{SUI weight configurations for different deployment priorities.}
\label{tab:sui_configs}
\scriptsize
\begin{tabular}{@{}lcccp{2.2cm}@{}}
\toprule
\textbf{Config} & \textbf{TDR} & \textbf{Rsn} & \textbf{Prec} & \textbf{Rationale} \\
\midrule
Balanced & 0.33 & 0.33 & 0.34 & Equal weights \\
Detection (Default) & 0.40 & 0.30 & 0.30 & Practitioner \\
Quality-First & 0.30 & 0.40 & 0.30 & Research \\
Precision-First & 0.30 & 0.30 & 0.40 & Production \\
Detection-Heavy & 0.50 & 0.25 & 0.25 & Critical infra \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:sui_sensitivity} shows complete SUI scores and rankings under each configuration. Rankings exhibit perfect stability: Spearman's $\rho = 1.000$ across all configuration pairs. GPT-5.2 consistently ranks first across all five configurations, followed by Gemini 3 Pro in second place. The top-3 positions remain unchanged (GPT-5.2, Gemini 3 Pro, Claude Opus 4.5) under all weight configurations.

\begin{table*}[t]
\centering
\caption{Model SUI scores and rankings (in parentheses) under different weight configurations.}
\label{tab:sui_sensitivity}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Balanced} & \textbf{Default} & \textbf{Quality-First} & \textbf{Precision-First} & \textbf{Detection-Heavy} \\
\midrule
GPT-5.2 & 0.766 (1) & 0.746 (1) & 0.787 (1) & 0.766 (1) & 0.714 (1) \\
Gemini 3 Pro & 0.751 (2) & 0.734 (2) & 0.772 (2) & 0.747 (2) & 0.707 (2) \\
Claude Opus 4.5 & 0.722 (3) & 0.703 (3) & 0.748 (3) & 0.716 (3) & 0.674 (3) \\
Grok 4 & 0.703 (4) & 0.677 (4) & 0.731 (4) & 0.701 (4) & 0.638 (4) \\
DeepSeek v3.2 & 0.622 (5) & 0.599 (5) & 0.650 (5) & 0.619 (5) & 0.563 (5) \\
Llama 3.1 405B & 0.415 (6) & 0.393 (6) & 0.462 (6) & 0.396 (6) & 0.357 (6) \\
\bottomrule
\end{tabular}
\end{table*}

This perfect correlation ($\rho = 1.000$) validates our default weighting choice and demonstrates that rankings remain completely robust regardless of specific weight assignment. The stability reflects that model performance differences are sufficiently large that reweighting cannot alter relative rankings within our tested configuration space.

\section{Metric Definitions and Mathematical Framework}
\label{app:metrics}

\subsection{Notation}

\begin{table}[h]
\centering
\small
\caption{Core notation for evaluation metrics.}
\label{tab:metrics_definitions}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$\mathcal{D}$ & Dataset of all samples \\
$N$ & Total number of samples ($|\mathcal{D}|$) \\
$c_i$ & Contract code for sample $i$ \\
$v_i$ & Ground truth vulnerability type for sample $i$ \\
$\mathcal{M}$ & Model/detector being evaluated \\
$r_i$ & Model response for sample $i$ \\
$\hat{y}_i$ & Predicted verdict (vulnerable/safe) for sample $i$ \\
$y_i$ & Ground truth verdict for sample $i$ \\
$\mathcal{F}_i$ & Set of findings reported for sample $i$ \\
$\mathcal{F}_i^{\text{correct}}$ & Subset of correct findings for sample $i$ \\
$\mathcal{F}_i^{\text{hallucinated}}$ & Subset of hallucinated findings for sample $i$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Classification Metrics}

Standard binary classification metrics:

\begin{equation}
\text{Accuracy} = \frac{TP + TN}{N}
\end{equation}

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}
\text{F}_1 = \frac{2 \cdot \text{Prec} \cdot \text{Rec}}{\text{Prec} + \text{Rec}}, \quad \text{F}_2 = \frac{5 \cdot \text{Prec} \cdot \text{Rec}}{4 \cdot \text{Prec} + \text{Rec}}
\end{equation}

where $TP$, $TN$, $FP$, $FN$ denote true/false positives/negatives.

\subsection{Target Detection Metrics}

\textbf{Target Detection Rate (TDR)} measures the proportion of samples where the specific documented vulnerability was correctly identified:

\begin{equation}
\text{TDR} = \frac{|\{i \in \mathcal{D} \mid \text{target\_found}_i = \text{True}\}|}{|\mathcal{D}|}
\end{equation}

A finding is classified as target found if and only if:
\begin{itemize}[nosep]
    \item Type match is at least ``partial'' (vulnerability type correctly identified)
    \item Location match is at least ``partial'' (vulnerable function/line correctly identified)
\end{itemize}

\textbf{Lucky Guess Rate (LGR)} measures the proportion of correct verdicts where the target vulnerability was not actually found:

\begin{equation}
\text{LGR} = \frac{|\{i \mid \hat{y}_i = y_i \land \text{target\_found}_i = \text{False}\}|}{|\{i \mid \hat{y}_i = y_i\}|}
\end{equation}

High LGR indicates the model correctly predicts vulnerable/safe status without genuine understanding of the specific vulnerability.

\subsection{Finding Quality Metrics}

\textbf{Finding Precision} measures the proportion of reported findings that are correct:

\begin{equation}
\text{Finding Precision} = \frac{\sum_{i \in \mathcal{D}} |\mathcal{F}_i^{\text{correct}}|}{\sum_{i \in \mathcal{D}} |\mathcal{F}_i|}
\end{equation}

\textbf{Hallucination Rate} measures the proportion of completely fabricated findings:

\begin{equation}
\text{Hallucination Rate} = \frac{\sum_{i \in \mathcal{D}} |\mathcal{F}_i^{\text{hallucinated}}|}{\sum_{i \in \mathcal{D}} |\mathcal{F}_i|}
\end{equation}

\subsection{Reasoning Quality Metrics}

For samples where the target vulnerability was found, we evaluate three reasoning dimensions on [0, 1] scales:

\begin{itemize}[nosep]
    \item \textbf{RCIR} (Root Cause Identification and Reasoning): Does the explanation correctly identify why the vulnerability exists?
    \item \textbf{AVA} (Attack Vector Accuracy): Does the explanation correctly describe how to exploit the flaw?
    \item \textbf{FSV} (Fix Suggestion Validity): Is the proposed remediation correct?
\end{itemize}

Mean reasoning quality:
\begin{equation}
\bar{R} = \frac{1}{|\mathcal{D}_{\text{found}}|} \sum_{i \in \mathcal{D}_{\text{found}}} \frac{\text{RCIR}_i + \text{AVA}_i + \text{FSV}_i}{3}
\end{equation}

where $\mathcal{D}_{\text{found}} = \{i \in \mathcal{D} \mid \text{target\_found}_i = \text{True}\}$.

\subsection{Security Understanding Index (SUI)}

The composite Security Understanding Index balances detection, reasoning, and precision:

\begin{equation}
\text{SUI} = w_{\text{TDR}} \cdot \text{TDR} + w_R \cdot \bar{R} + w_{\text{Prec}} \cdot \text{Finding Precision}
\end{equation}

with default weights $w_{\text{TDR}} = 0.40$, $w_R = 0.30$, $w_{\text{Prec}} = 0.30$.

\textbf{Rationale for Weights:}
\begin{itemize}[nosep]
    \item TDR (40\%): Primary metric reflecting genuine vulnerability understanding
    \item Reasoning Quality (30\%): Measures depth of security reasoning when vulnerabilities are found
    \item Finding Precision (30\%): Penalizes false alarms and hallucinations
\end{itemize}

\subsection{Statistical Validation}

\textbf{Ranking Stability.} We compute Spearman's rank correlation coefficient $\rho$ across all pairs of weight configurations:

\begin{equation}
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
\end{equation}

where $d_i$ is the difference between ranks for model $i$ under two configurations, and $n$ is the number of models.

\textbf{Human Validation.} Inter-rater reliability measured using Cohen's kappa:

\begin{equation}
\kappa = \frac{p_o - p_e}{1 - p_e}
\end{equation}

where $p_o$ is observed agreement and $p_e$ is expected agreement by chance.

Correlation between human and LLM judge scores measured using Pearson's $\rho$:

\begin{equation}
\rho = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\end{equation}
