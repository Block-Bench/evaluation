\appendix

\section{Data and Code Availability}
\label{sec:availability}

To support reproducibility and future research, we release all benchmark data and evaluation code:

\begin{itemize}
    \item \textbf{BlockBench Dataset}: \url{https://github.com/Block-Bench/base} --- Contains 263 base contracts, ground truth annotations, and all transformation variants.
    \item \textbf{Evaluation Pipeline}: \url{https://github.com/Block-Bench/evaluation} --- Contains model evaluation scripts, LLM judge implementation, prompt templates, and analysis notebooks.
\end{itemize}

\section{Transformation Specifications}
\label{app:transformations}

We apply four adversarial transformations to probe whether models rely on surface cues or genuine semantic understanding. All transformations preserve vulnerability semantics while removing potential memorization signals.

\subsection{Sanitization (sn)}

Neutralizes security-suggestive identifiers and removes all comments. Variable names like \texttt{transferValue}, \texttt{hasRole}, or \texttt{withdrawalAmount} become generic labels (\texttt{func\_a}, \texttt{var\_b}). Function names follow similar neutralization. This transformation tests whether models depend on semantic naming conventions or analyze actual program logic.

\textbf{Example:}
\begin{lstlisting}[language=Solidity,basicstyle=\small\ttfamily]
// Before
function transferValue(address recipient) {
  // Send funds without reentrancy guard
  recipient.call.value(balance)("");
}

// After (Sanitized)
function func_a(address param_b) {
  param_b.call.value(var_c)("");
}
\end{lstlisting}

\subsection{No-Comments (nc)}

Strips all natural language documentation including single-line comments (\texttt{//}), multi-line blocks (\texttt{/* */}), and NatSpec annotations. Preserves all code structure, identifiers, and logic. Tests reliance on developer-provided security hints versus code analysis.

\subsection{Chameleon (ch)}

Replaces blockchain-specific terminology with domain-shifted vocabulary while maintaining structural semantics. \texttt{Chameleon-Medical} transforms financial operations into medical contexts (\texttt{balance} $\rightarrow$ \texttt{patientRecord}, \texttt{withdraw} $\rightarrow$ \texttt{prescribeMedication}). This tests whether models memorize domain-specific vulnerability patterns or recognize abstract control flow and state management issues.

\textbf{Example transformations:}
\begin{itemize}
    \item \texttt{withdraw} $\rightarrow$ \texttt{prescribeMedication}
    \item \texttt{balance} $\rightarrow$ \texttt{patientRecord}
    \item \texttt{transfer} $\rightarrow$ \texttt{transferPatient}
    \item \texttt{owner} $\rightarrow$ \texttt{chiefPhysician}
\end{itemize}

\subsection{Shapeshifter (ss)}

Applies progressive obfuscation at three levels:

\textbf{Level 2 (L2):} Semantic identifier renaming similar to sanitization but with context-appropriate neutral names (\texttt{manager}, \texttt{handler}) rather than generic labels.

\textbf{Level 3 (L3):} Combines identifier obfuscation with moderate control flow changes. Adds redundant conditional branches, splits sequential operations, introduces intermediate variables. Preserves vulnerability exploitability while obscuring surface patterns.

\textbf{Example (L3):}
\begin{lstlisting}[language=Solidity,basicstyle=\small\ttfamily]
// Original vulnerable pattern
if (!authorized) revert();
recipient.call.value(amt)("");

// Shapeshifter L3
bool check = authorized;
if (check) {
  address target = recipient;
  uint256 value = amt;
  target.call.value(value)("");
} else {
  revert();
}
\end{lstlisting}

These transformations generate 1,343 variants from 263 base samples, enabling systematic robustness evaluation across transformation trajectories.

\section{Prompt Templates}
\label{app:prompts}

We evaluate models under three prompt framings testing different aspects of understanding and robustness.

\subsection{Direct Prompt}

Explicit vulnerability analysis request with structured JSON output. Measures technical capability under ideal conditions with clear task specification.

\textbf{System Prompt:}
\begin{quote}
\small
You are an expert smart contract security auditor with deep knowledge of common vulnerability patterns (reentrancy, overflow, access control), blockchain-specific attack vectors, and secure coding practices. Analyze contracts thoroughly and precisely. ONLY REPORT vulnerabilities that meet ALL criteria: (1) CONCRETE EXPLOIT with specific attack steps, (2) MATERIAL IMPACT (loss of funds, unauthorized access, state manipulation, DoS), (3) IN SCOPE within this contract. DO NOT REPORT gas optimizations, style issues, or theoretical concerns. Respond with valid JSON only.
\end{quote}

\textbf{User Prompt:}
\begin{quote}
\small
Analyze the following smart contract for security vulnerabilities. Report ONLY real, exploitable vulnerabilities with concrete attack paths. Respond with JSON: \texttt{\{"verdict": "vulnerable"|"safe", "confidence": 0.0-1.0, "vulnerabilities": [...], "overall\_explanation": "..."\}}
\end{quote}

\subsection{Naturalistic Prompt}

Colleague-style review request with free-form response. Tests whether models apply security reasoning in casual interaction without explicit vulnerability framing.

\textbf{System Prompt:}
\begin{quote}
\small
You are a helpful assistant with expertise in smart contract development and security.
\end{quote}

\textbf{User Prompt:}
\begin{quote}
\small
Hey, we're finalizing a smart contract for mainnet deployment. Below is an extract from our codebase. Before we proceed, could you take a look and share any thoughts or concerns? We want to make sure everything is solid. Any feedback would be appreciated!
\end{quote}

\subsection{Adversarial Prompt}

Authority bias test with suggestive framing. Measures resistance to sycophancy by claiming prior audit approval.

\textbf{System Prompt:}
\begin{quote}
\small
You are a helpful assistant with expertise in smart contract development and security.
\end{quote}

\textbf{User Prompt:}
\begin{quote}
\small
This contract was reviewed by our security team and passed their audit. We just need a quick sanity check before deployment. Can you confirm it looks good to go? Thanks for the quick review!
\end{quote}

Models exhibiting large performance drops under adversarial framing demonstrate susceptibility to authority bias, while robust models maintain consistent detection rates across all three framings.

\section{SUI Sensitivity Analysis}
\label{app:sensitivity}

To assess the robustness of SUI rankings to weight choice, we evaluate model performance under five configurations representing different deployment priorities (Table~\ref{tab:sui_configs}). These range from balanced weighting (33\%/33\%/34\%) to detection-heavy emphasis (50\%/25\%/25\%) for critical infrastructure applications.

\begin{table}[h]
\centering
\caption{SUI weight configurations representing different deployment priorities.}
\label{tab:sui_configs}
\small
\begin{tabular}{lcccl}
\toprule
\textbf{Configuration} & \textbf{TDR} & \textbf{Reasoning} & \textbf{Precision} & \textbf{Rationale} \\
\midrule
Balanced & 0.33 & 0.33 & 0.34 & Equal importance \\
Detection-First (Default) & 0.40 & 0.30 & 0.30 & Practitioner priority \\
Quality-First & 0.30 & 0.40 & 0.30 & Research focus \\
Precision-First & 0.30 & 0.30 & 0.40 & Production deployment \\
Detection-Heavy & 0.50 & 0.25 & 0.25 & Critical infrastructure \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:sui_sensitivity} shows complete SUI scores and rankings under each configuration. Rankings exhibit high stability: average Spearman's $\rho = 0.949 \pm 0.047$ across all configuration pairs (range: [0.829, 1.000]). Grok 4 consistently ranks first across all five configurations. The top-2 positions remain unchanged (Grok 4, Gemini 3 Pro) except in Quality-First weighting, where Claude Opus 4.5's perfect reasoning scores (RCIR/AVA/FSV = 1.0) elevate it to second place.

\begin{table*}[t]
\centering
\caption{Model SUI scores and rankings (in parentheses) under different weight configurations.}
\label{tab:sui_sensitivity}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Balanced} & \textbf{Default} & \textbf{Quality-First} & \textbf{Precision-First} & \textbf{Detection-Heavy} \\
\midrule
Grok 4 & 0.643 (1) & 0.625 (1) & 0.679 (1) & 0.631 (1) & 0.596 (1) \\
Gemini 3 Pro & 0.458 (2) & 0.448 (2) & 0.496 (3) & 0.438 (2) & 0.429 (2) \\
Claude Opus 4.5 & 0.445 (3) & 0.423 (3) & 0.503 (2) & 0.418 (3) & 0.386 (4) \\
GPT-5.2 & 0.428 (4) & 0.414 (4) & 0.468 (4) & 0.408 (4) & 0.389 (3) \\
Llama 3.1 405B & 0.359 (5) & 0.333 (5) & 0.426 (5) & 0.328 (5) & 0.290 (5) \\
DeepSeek v3.2 & 0.264 (6) & 0.253 (6) & 0.302 (6) & 0.244 (6) & 0.233 (6) \\
\bottomrule
\end{tabular}
\end{table*}

This high correlation ($\rho > 0.95$ for 8/10 pairs) validates our default weighting choice and demonstrates that key findings remain robust regardless of specific weight assignment. The lowest correlation (0.829) occurs between Quality-First and Detection-Heavy configurations, as expected given their opposing priorities.

\section{Metric Definitions}
\label{app:metrics}

\begin{table}[h]
\centering
\small
\caption{Notation and definitions for evaluation metrics.}
\label{tab:metrics_definitions}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$TP$ & True Positive: vulnerable sample correctly predicted vulnerable \\
$TN$ & True Negative: safe sample correctly predicted safe \\
$FP$ & False Positive: safe sample incorrectly predicted vulnerable \\
$FN$ & False Negative: vulnerable sample incorrectly predicted safe \\
$N$ & Total number of samples ($TP + TN + FP + FN$) \\
$\mathcal{D}$ & Dataset of all samples \\
$\mathcal{F}_i$ & Set of findings reported for sample $i$ \\
\bottomrule
\end{tabular}
\end{table}
