\section{Results}

We evaluate six frontier models on 58 Solidity vulnerability samples across Temporal Contamination (TC), Gold Standard (GS), and Difficulty Stratified (DS) subsets covering 11 vulnerability types.

\subsection{Overall Performance}

Table~\ref{tab:overall_results} presents aggregate performance ranked by Target Detection Rate (TDR), our primary metric measuring correct vulnerability identification. Grok 4 achieves highest detection (45\%), yet misses over half of vulnerabilities. Llama's 43\% accuracy conceals catastrophic 7\% TDR with 83\% lucky guesses,  revealing correct vulnerable classification without identifying specific flaw types or locations. Claude, Llama, and Grok demonstrate superior reasoning quality (0.97--1.00) when successfully identifying targets, providing comprehensive explanations of root causes and attack vectors. GPT-5.2 shows lowest lucky guess rate (25\%), indicating higher reliability when flagging vulnerabilities.

\begin{table*}[t]
\centering
\small
\caption{Overall performance ranked by Target Detection Rate. Best values bold.}
\label{tab:overall_results}
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\textbf{Model} & \textbf{TDR} & \textbf{SUI} & \textbf{Acc} & \textbf{Prec} & \textbf{Lucky\%} & \textbf{RCIR} & \textbf{AVA} & \textbf{FSV} & \textbf{Hall\%} & \textbf{Findings} \\
\midrule
Grok 4 & \textbf{44.8} & \textbf{0.625} & 68.7 & \textbf{50.3} & 41.3 & 0.98 & \textbf{1.00} & 0.97 & 1.4 & 2.16 \\
Gemini 3 Pro & 33.3 & 0.448 & \textbf{73.3} & 23.2 & 54.5 & 0.85 & 0.80 & 0.80 & \textbf{0.0} & 3.73 \\
GPT-5.2 & 26.7 & 0.414 & 26.7 & 21.1 & \textbf{25.0} & 0.88 & 0.81 & 0.75 & \textbf{0.0} & 4.73 \\
Claude Opus 4.5 & 20.0 & 0.423 & 40.0 & 14.4 & 50.0 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{0.0} & 6.00 \\
DeepSeek v3.2 & 13.3 & 0.253 & 26.7 & 4.1 & 75.0 & 0.75 & 0.62 & 0.50 & 4.1 & 4.87 \\
Llama 3.1 405B & 7.1 & 0.333 & 42.9 & 1.6 & 83.3 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 1.6 & 4.50 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Gold Standard and Robustness Analysis}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{figures/gs_performance.png}
\caption{Gold Standard performance degradation. All models experience 36-50pp TDR drops on post-September 2025 samples.}
\label{fig:gs_performance}
\end{figure}

Gold Standard samples from post-September 2025 audits reveal dramatic performance degradation (Figure~\ref{fig:gs_performance}). All models experience 36-50 percentage point TDR drops compared to overall performance. Grok 4 drops from 45\% to 32\% TDR, Gemini from 33\% to 26\%, and Claude from 20\% to 11\%. This degradation confirms that models struggle with truly unseen vulnerabilities from professional audits.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{figures/lucky_guess.png}
\caption{Lucky guess rates by model. High rates indicate correct verdicts without target identification.}
\label{fig:lucky_guess}
\end{figure}

Sanitization experiments expose reliance on lexical cues. When semantic variable names are neutralized (transferValue → func_a), top models experience catastrophic degradation. Claude drops from 100\% to 56\% accuracy (TDR 86\%→24\%), GPT-5.2 from 100\% to 44\% (TDR 86\%→32\%), and Gemini from 100\% to 83\% (TDR 86\%→29\%). Identical program logic with neutral naming causes accuracy collapses of 40-60 percentage points.

Lucky guessing (Figure~\ref{fig:lucky_guess}) reveals models classify vulnerabilities correctly while misidentifying specific flaws. Llama achieves 83\% lucky guesses, DeepSeek 75\%, indicating pattern recognition without precise understanding. Domain shift transformations replacing blockchain terminology with medical vocabulary show minimal impact. Top models maintain 100\% accuracy with 58-73\% TDR, suggesting models learn structural patterns beyond domain-specific tokens.

Prompt framing reveals inconsistent robustness. Adversarial prompts claiming prior audit approval cause complete detection collapse in some models (Grok, Llama TDR 40\%→0\%) while improving others (Claude precision 6\%→25\%).

\subsection{Human Validation}

Two security experts independently reviewed 20 responses, achieving substantial inter-rater agreement (verdict κ=0.91, type match κ=0.84, reasoning κ=0.78). Human assessments strongly correlated with LLM judge scores (ρ=0.87, p<0.001) with 85\% decision agreement, validating automated evaluation reliability.
