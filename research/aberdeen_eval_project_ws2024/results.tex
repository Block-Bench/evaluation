\section{Results}

We evaluate six frontier models on 58 Solidity vulnerability samples across Temporal Contamination (TC), Gold Standard (GS), and Difficulty Stratified (DS) subsets.

\subsection{Overall Performance}

\begin{table*}[!ht]
\centering
\small
\caption{Overall performance ranked by Target Detection Rate. Best values bold.}
\label{tab:overall_results}
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Model} & \textbf{TDR} & \textbf{SUI} & \textbf{Acc} & \textbf{RCIR} & \textbf{AVA} & \textbf{FSV} & \textbf{Findings} \\
\midrule
Gemini 3 Pro & \textbf{57.6} & 0.734 & \textbf{93.9} & 0.97 & 0.97 & 0.95 & 2.6 \\
GPT-5.2 & 55.9 & \textbf{0.746} & 75.0 & 0.97 & 0.98 & \textbf{0.97} & 2.4 \\
Claude Opus 4.5 & 52.9 & 0.703 & 83.8 & 0.98 & 0.99 & 0.97 & 3.5 \\
Grok 4 & 44.1 & 0.677 & 69.1 & \textbf{0.98} & \textbf{1.00} & \textbf{0.97} & 2.1 \\
DeepSeek v3.2 & 38.2 & 0.599 & 82.4 & 0.91 & 0.92 & 0.86 & 3.0 \\
Llama 3.1 405B & 17.9 & 0.393 & \textbf{88.1} & 0.88 & 0.90 & 0.83 & 2.0 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\columnwidth]{figures/tdr_comparison.png}
\caption{Target Detection Rate across all models. Best performer achieves 58\% detection, while highest accuracy (88\%) corresponds to lowest TDR (18\%).}
\label{fig:tdr_comparison}
\end{figure}

Table~\ref{tab:overall_results} and Figure~\ref{fig:tdr_comparison} present aggregate performance ranked by Target Detection Rate (TDR). Gemini 3 Pro achieves highest detection (58\%), followed by GPT-5.2 (56\%) and Claude Opus 4.5 (53\%). When combining detection, reasoning quality, and finding precision into the Security Understanding Index (SUI), GPT-5.2 ranks first (0.746) due to superior finding precision (77\%), followed by Gemini 3 Pro (0.734).

Llama 3.1 405B exhibits the most severe accuracy-TDR gap: 88\% accuracy yet only 18\% TDR, correctly classifying vulnerable samples without identifying specific vulnerability types or locations. This 70-percentage-point discrepancy demonstrates that binary classification metrics inadequately measure security understanding.

All models achieving target detection show strong reasoning quality (RCIR/AVA/FSV $\geq$0.95), with minimal variation in explanation quality across top performers.

\subsection{Gold Standard Performance}

Gold Standard samples from post-September 2025 audits guarantee zero temporal contamination. Performance drops substantially: Claude Opus 4.5 leads with 20\% TDR, followed by Gemini 3 Pro (11\%), GPT-5.2 (10\%), and Grok 4 (10\%). DeepSeek v3.2 and Llama 3.1 405B detect zero targets. All models experience 34-50 percentage point drops from overall to Gold Standard performance.

\subsection{Transformation Robustness}

\textbf{Sanitization.} Neutralizing security-suggestive identifiers causes variable degradation. On temporal contamination samples, top models achieve 60\% TDR on baseline versions. Sanitization impacts differ: GPT-5.2 and DeepSeek v3.2 maintain performance, while Grok 4 drops 40pp, exposing varying reliance on lexical cues.

\textbf{Domain Shift.} Replacing blockchain terminology with medical vocabulary shows mixed impact. Performance ranges 20-60\% TDR across models, with GPT-5.2 maintaining 60\% detection while others show 20-50\% degradation.

\textbf{Prompt Framing.} Performance varies significantly across direct, adversarial (claiming prior audit approval), and naturalistic prompts. Gemini 3 Pro and GPT-5.2 demonstrate robustness with 18-21pp drops from direct to non-direct prompts. Claude Opus 4.5 and DeepSeek v3.2 show larger degradation (21-39pp), while Llama 3.1 405B exhibits inconsistent behavior (adversarial: 0\%, naturalistic: 25\%).

\subsection{Human Validation}

Two security experts independently reviewed 20 responses. Inter-rater agreement: verdict $\kappa$=0.91, type match $\kappa$=0.84, reasoning $\kappa$=0.78. Human-judge correlation: $\rho$=0.87 (p<0.001), 85\% agreement, validating automated evaluation.
