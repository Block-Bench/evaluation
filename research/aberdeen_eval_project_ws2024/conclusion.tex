\section{Conclusion}

BlockBench evaluates whether frontier LLMs genuinely understand smart contract vulnerabilities or merely pattern-match against training data. Our assessment of six models on 263 Solidity samples reveals substantial limitations. Best performance reaches 58\% detection on mixed samples, collapsing to 20\% on uncontaminated Gold Standard audits. Llama 3.1 405B achieves 88\% accuracy yet only 18\% TDR, demonstrating that binary classification metrics inadequately measure security understanding.

Models exhibit heterogeneous robustness. While GPT-5.2 maintains stable performance across transformations, most models degrade when surface cues are removed, indicating incomplete semantic abstraction. Current frontier LLMs cannot serve as autonomous auditors but show promise in ensemble workflows with mandatory expert review. Future work should develop sanitization-resistant evaluation methods and explore hybrid LLM-verification architectures.

\paragraph{AI Assistance.} Claude Sonnet 4.5 assisted with evaluation pipeline code and manuscript refinement. All research design, experimentation, and analysis were conducted by the authors.
