\section{Related Work}

Traditional static and dynamic analysis tools including Slither \citep{feist2019slither}, Mythril \citep{mueller2017mythril}, and Securify \citep{tsankov2018securify} detect only 27-42\% of known vulnerabilities in annotated datasets while flagging 97\% of 47,587 real-world Ethereum contracts as vulnerable \citep{durieux2020empirical}. Recent LLM-based approaches \citep{hu2023gptlens,liu2024propertygpt} show promise, with fine-tuned models achieving over 90\% accuracy on benchmarks \citep{hossain2025leveraging}, though performance degrades on real-world contracts \citep{ince2025gendetect}. Existing benchmarks like SmartBugs Curated \citep{ferreira2020smartbugs} primarily evaluate detection accuracy without distinguishing genuine understanding from memorization. Models exhibit high sensitivity to input modifications \citep{sanchez2025none,wu2024reasoning}, suggesting pattern memorization. Our work extends robustness evaluation to blockchain security through systematic transformations probing genuine understanding versus memorized patterns. See Appendix~\ref{app:related} for detailed survey.
