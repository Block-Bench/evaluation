\section{Discussion}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{../../analysis_results/transformation_analysis/sui_trajectory.png}
\caption{Security Understanding Index trajectory across progressive transformations of TC samples. GPT-5.2 maintains near-constant performance while most models degrade, revealing varying degrees of surface pattern reliance.}
\label{fig:sui_trajectory}
\end{figure}

\textbf{Understanding versus Memorization.}
Figure~\ref{fig:sui_trajectory} reveals heterogeneous robustness across models. GPT-5.2 maintains stable SUI (78.0→77.2) through sanitization, domain shifts, and obfuscation, demonstrating genuine semantic understanding. In contrast, DeepSeek v3.2 degrades 19.7 points (78.7→59.0), indicating surface pattern dependence. Most models exhibit intermediate behavior, leveraging lexical cues when available while retaining partial structural understanding \citep{chen2021codex,wu2024reasoning}. This heterogeneity suggests current training methods produce inconsistent abstraction capabilities across architectures \citep{sanchez2025none}. While genuine security understanding is demonstrably possible, most frontier models have not achieved it.

\textbf{Measurement Inadequacy.}
The accuracy-TDR gap exposes fundamental metric limitations. Llama 3.1 405B achieves 88\% accuracy yet only 18\% TDR, correctly classifying samples as vulnerable without identifying specific flaw types or locations \citep{jimenez2024swebench}. For security practitioners requiring actionable findings, binary classification provides insufficient value. Effective evaluation must measure precise vulnerability localization, not merely anomaly detection.

\textbf{Practical Implications.}
Current frontier models cannot serve as autonomous auditors. Best performance reaches 58\% detection with substantial Gold Standard degradation (20\% maximum). However, complementary strengths suggest ensemble potential: Grok 4 offers breadth, GPT-5.2 provides consistency, Claude delivers explanation quality. Workflows positioning LLMs as assistive tools with mandatory expert review align capabilities with current limitations \citep{hu2023gptlens,ince2025gendetect}.

\textbf{Ethical Considerations.}
BlockBench poses dual-use risks: adversarial prompts demonstrate methods that could suppress detection, while detailed vulnerability documentation may assist malicious actors. We justify public release on several grounds: adversarial robustness represents a fundamental requirement for security tools, malicious actors will discover these vulnerabilities regardless, and responsible disclosure enables proactive mitigation. All samples derive from already-disclosed vulnerabilities and public security audits, ensuring no novel exploit information is revealed. Practitioners should avoid over-reliance on imperfect tools, as false negatives create security gaps while false confidence may reduce manual review rigor.

\textbf{Limitations and Future Work.}
Our evaluation uses 263 samples with 10 Gold Standard examples. We assess zero-shot prompting exclusively. Chain-of-thought reasoning or retrieval augmentation may improve performance. Future work should expand sample diversity across blockchain ecosystems, develop sanitization-resistant analysis using control flow graphs, and explore hybrid LLM-verification architectures \citep{liu2024propertygpt}.
