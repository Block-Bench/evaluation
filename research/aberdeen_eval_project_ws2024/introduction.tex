\section{Introduction}

Smart contract vulnerabilities represent one of the costliest security challenges in computing, with cryptocurrency theft exceeding \$14 billion since 2020, reaching \$3.4 billion in 2025 \citep{chainalysis2025}. Individual incidents like Bybit (\$1.5B) and Cetus (\$223M from overflow) demonstrate catastrophic impact.

Frontier LLMs achieve remarkable programming performance, yet their blockchain security capabilities remain unclear. Can these models genuinely reason about vulnerabilities, or merely pattern-match memorized examples? This distinction matters. Models memorizing the 2016 DAO attack may recognize similar patterns yet fail when identical logic appears in unfamiliar syntax.

We introduce \textbf{BlockBench}, a benchmark distinguishing understanding from memorization. Our contributions are: (1) 263 Solidity samples across Difficulty Stratified, Temporal Contamination, and Gold Standard subsets enabling complexity, memorization, and uncontaminated evaluation; (2) novel metrics including Target Detection Rate and lucky guesses exposing accuracy-understanding gaps; (3) evaluation of six frontier models revealing best performance at 45\% detection with sanitization causing 40-60pp drops.
