\section{Related Work}

\subsection{Traditional Analysis Tools}

Early approaches to smart contract vulnerability detection relied on static analysis and symbolic execution. Tools such as Slither \citep{feist2019slither}, Mythril \citep{mueller2017mythril}, and Securify \citep{tsankov2018securify} demonstrated strong precision on syntactically well-defined vulnerability classes. \citet{durieux2020empirical} conducted a comprehensive evaluation of nine such tools across 47,587 Ethereum contracts, revealing consistent performance on reentrancy and integer overflow detection, yet persistent struggles with vulnerabilities requiring semantic reasoning about contract logic. \citet{ghaleb2020solidifi} corroborated these findings, observing that rule-based approaches fundamentally cannot capture the contextual nuances that distinguish exploitable flaws from benign code patterns.

\subsection{LLM-Based Approaches}

Large language models introduced new possibilities for bridging this semantic gap. Initial investigations by \citet{chen2023chatgpt} explored prompting strategies for vulnerability detection, achieving detection rates near 40\% while noting pronounced sensitivity to superficial features such as variable naming conventions. GPTScan \citep{sun2023gptscan} combined GPT-4 with program analysis to achieve 78\% precision on logic vulnerabilities, leveraging static analysis to validate LLM-generated candidates. \citet{sun2024llm4vuln} introduced retrieval-augmented approaches that provide models with relevant vulnerability descriptions, substantially improving detection performance. Multi-agent architectures emerged as another direction, with systems like GPTLens \citep{hu2023gptlens} employing auditor-critic pairs to enhance analytical consistency. Fine-tuning on domain-specific corpora has yielded incremental gains, though performance characteristically plateaus below the 85\% threshold regardless of training scale.

\subsection{Pattern Recognition Versus Understanding}

Beneath these encouraging metrics lies a more fundamental question: whether observed improvements reflect genuine comprehension of vulnerability mechanics or increasingly sophisticated pattern recognition. Several empirical observations suggest the latter warrants serious consideration. \citet{sun2024llm4vuln} demonstrated that decoupling vulnerability descriptions from code context precipitates catastrophic performance degradation, indicating that models may rely on memorized associations between textual cues and vulnerability labels rather than reasoning about exploit mechanics. \citet{hu2023gptlens} observed that models produce divergent outputs for identical queries even at temperature zero, a phenomenon difficult to reconcile with deterministic security reasoning. \citet{wu2024reasoning} showed through counterfactual tasks in adjacent domains that language models systematically fail when familiar patterns are disrupted, defaulting to memorized responses rather than applying causal logic to novel configurations.

\subsection{Evaluation Methodology}

The distinction between pattern recognition and genuine understanding carries profound implications for security applications, where adversarial actors actively craft exploits to evade detection. A model that has memorized the surface features of known vulnerabilities provides little defense against novel attack vectors or obfuscated variants of familiar exploits. Existing benchmarks such as SmartBugs Curated \citep{durieux2020empirical} and DeFiVulnLabs \citep{defivulnlabs2023} assess binary detection outcomes without examining whether models can identify specific code elements that enable exploitation, distinguish genuine vulnerabilities from superficially suspicious but benign patterns, or maintain accuracy when surface-level cues are systematically removed. Our work contributes evaluation methodology that directly probes this distinction through adversarial transformations preserving vulnerability semantics while removing surface cues.
