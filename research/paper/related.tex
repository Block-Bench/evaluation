\section{Related Work}

\subsection{Traditional Analysis Tools}

Early approaches to smart contract vulnerability detection relied on static analysis and symbolic execution. Tools such as Slither \citep{feist2019slither}, Mythril \citep{mueller2017mythril}, and Securify \citep{tsankov2018securify} demonstrated strong precision on syntactically well-defined vulnerability classes. \citet{durieux2020empirical} evaluated nine tools across 47,587 contracts, finding 27--42\% detection rates with 97\% of contracts flagged as vulnerable, indicating impractically high false positive rates. Recent comparison confirms LLMs are ``not ready to replace'' traditional analyzers, though tools exhibit complementary strengths: traditional analyzers excel on reentrancy while LLMs show advantages on complex logic errors \citep{ince2025gendetect}.

\subsection{LLM-Based Approaches}

Large language models introduced new possibilities for bridging this semantic gap. Initial investigations by \citet{chen2023chatgpt} explored prompting strategies for vulnerability detection, achieving detection rates near 40\% while noting pronounced sensitivity to superficial features such as variable naming conventions. GPTScan \citep{sun2023gptscan} combined GPT-4 with program analysis to achieve 78\% precision on logic vulnerabilities, leveraging static analysis to validate LLM-generated candidates. \citet{sun2024llm4vuln} introduced retrieval-augmented approaches that provide models with relevant vulnerability descriptions, substantially improving detection performance. Multi-agent architectures emerged as another direction, with systems like GPTLens \citep{hu2023gptlens} employing auditor-critic pairs to enhance analytical consistency. Fine-tuning on domain-specific corpora has yielded incremental gains, though performance characteristically plateaus below the 85\% threshold regardless of training scale.

\subsection{Pattern Recognition Versus Understanding}

Beneath these encouraging metrics lies a more fundamental question: whether observed improvements reflect genuine comprehension of vulnerability mechanics or increasingly sophisticated pattern recognition. Several empirical observations suggest the latter warrants serious consideration. \citet{sun2024llm4vuln} demonstrated that decoupling vulnerability descriptions from code context precipitates catastrophic performance degradation, indicating that models may rely on memorized associations between textual cues and vulnerability labels rather than reasoning about exploit mechanics. \citet{hu2023gptlens} documented output drift where GPT-4 ``easily identified the vulnerability on September 16 but had difficulty detecting it on September 28'' with temperature zero, requiring few-shot examples to stabilize behavior. \citet{wu2024reasoning} showed through counterfactual tasks in adjacent domains that language models systematically fail when familiar patterns are disrupted, defaulting to memorized responses rather than applying causal logic to novel configurations.

\subsection{Can Current State-of-the-Art Do Better?}

This is the crux of our work: investigating whether frontier models released since these studies exhibit genuine security understanding or remain bound by the same pattern-matching limitations.
