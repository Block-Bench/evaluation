\section{Conclusion}

BlockBench evaluates whether frontier LLMs genuinely understand smart contract vulnerabilities or merely pattern-match. Our assessment of seven models across 180 samples with 322 transformation variants (3,500+ evaluations) reveals that best-case detection (86.5\% on DS) degrades sharply under adversarial conditions: 50.9\% on obfuscated variants, 25.3\% on uncontaminated post-cutoff samples.

The pattern matching paradox highlights a key limitation: models can locate vulnerable code without understanding why it is exploitable. Llama achieves highest \colorbox{red!25}{\textsc{Root\_Cause}} match (60.9\%) but lowest TDR (31.7\%), suggesting pattern memorization rather than causal reasoning. All models recognize \colorbox{green!20}{\textsc{Benign}} patterns (100\% fix recognition) more reliably than \colorbox{red!25}{\textsc{Root\_Cause}} segments, suggesting surface-level pattern matching dominates current approaches.

\textbf{Practical implications:} Current LLMs cannot serve as autonomous auditors. However, complementary model strengths suggest ensemble potential: Claude for detection quality, GPT-5.2 for precision (89.6\%), with prompt engineering yielding significant gains (+29pp adversarial framing). Effective deployment requires mandatory expert review and should leverage LLMs as assistive tools rather than replacements. Future work should develop contamination-resistant evaluation methods and hybrid architectures combining pattern recognition with formal verification.
