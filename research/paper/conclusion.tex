\section{Conclusion}

BlockBench evaluates whether frontier LLMs genuinely understand smart contract vulnerabilities or merely pattern-match. Our assessment of seven frontier models across 180 original samples with 322 transformation variants---yielding over 3,500 unique evaluations---reveals substantial limitations. Best performance reaches 86.5\% on DS (Claude), degrading to 50.9\% on TC variants and 25.3\% on Gold Standard post-cutoff samples.

The pattern matching paradox exposes a critical gap: Llama achieves highest \colorbox{red!25}{\textsc{Root\_Cause}} match (60.9\%) but lowest TDR (31.7\%), locating vulnerable code without articulating why it is exploitable. All models achieve 100\% fix recognition on differential variants, recognizing \colorbox{green!20}{\textsc{Benign}} patterns more reliably than understanding \colorbox{red!25}{\textsc{Root\_Cause}} segments.

Current frontier LLMs cannot serve as autonomous auditors but show promise in assistive workflows. Claude delivers highest detection and explanation quality, GPT-5.2 provides best precision (89.6\%), and prompt engineering yields significant gains (adversarial framing boosts Claude by 29pp). Future work should develop contamination-resistant methods and explore hybrid LLM-verification architectures that combine pattern recognition strengths with formal reasoning.
