\section{Results}

We evaluate seven frontier LLMs on BlockBench using 180 original samples (DS $n$=100, TC $n$=46, GS $n$=34) with 322 TC transformation variants, yielding over 3,500 unique model-sample evaluations. All detection results use majority voting across three LLM judges (GLM-4.7, MIMO-v2-Flash, Mistral-Large), where a target is marked as ``found'' only if $\geq$2 judges agree.

\subsection{Detection Performance}

\begin{table*}[!ht]
\centering
\small
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{@{}l|ccccc|ccccccc|c@{}}
\toprule
& \multicolumn{5}{c|}{\textbf{DS (Difficulty-Stratified)}} & \multicolumn{7}{c|}{\textbf{TC (Temporal Contamination)}} & \\
\textbf{Model} & \textbf{T1} & \textbf{T2} & \textbf{T3} & \textbf{T4} & \textbf{Avg [95\% CI]} & \textbf{MinS} & \textbf{San} & \textbf{NoC} & \textbf{Cha} & \textbf{Shp} & \textbf{Tro} & \textbf{FalP} & \textbf{Avg} \\
\midrule
Claude Opus 4.5 & \textbf{100} & \textbf{83.8} & \textbf{70.0} & 92.3 & \textbf{86.5}$^{a}$ \scriptsize{[82--91]} & \textbf{71.7} & \textbf{54.3} & \textbf{50.0} & \textbf{43.5} & \textbf{50.0} & 32.6 & \textbf{54.3} & \textbf{50.9} \\
Gemini 3 Pro & 75.0 & 78.4 & 50.0 & \textbf{92.3} & 73.9$^{a}$ \scriptsize{[68--80]} & 65.2 & 28.3 & 32.6 & 37.0 & 34.8 & 34.8 & 37.0 & 38.5 \\
GPT-5.2 & 60.0 & 70.3 & 36.7 & 84.6 & 62.9$^{a}$ \scriptsize{[56--70]} & 54.3 & 34.8 & 37.0 & 28.3 & 30.4 & 30.4 & 37.0 & 36.0 \\
DeepSeek v3.2 & 65.0 & 64.9 & 46.7 & 61.5 & 59.5 \scriptsize{[53--66]} & 58.7 & 37.0 & 41.3 & 21.7 & 26.1 & \textbf{43.5} & 30.4 & 37.0 \\
Llama 4 Mav & 65.0 & 45.9 & 40.0 & 69.2 & 55.0 \scriptsize{[48--62]} & 52.2 & 39.1 & 30.4 & 21.7 & 13.0 & \textbf{43.5} & 21.7 & 31.7 \\
Qwen3 Coder$^{b}$ & 60.0 & 56.8 & 43.3 & 53.8 & 53.5 \scriptsize{[47--60]} & 56.5 & 43.5 & 30.4 & 15.2 & 17.4 & 28.3 & 41.3 & 33.2 \\
Grok 4$^{b}$ & 40.0 & 37.8 & 33.3 & 30.8 & 35.5 \scriptsize{[29--42]} & 32.6 & 23.9 & 19.6 & 15.2 & 15.2 & 21.7 & 21.7 & 21.4 \\
\bottomrule
\end{tabular}
\caption{Target Detection Rate (\%) on DS and TC benchmarks using majority vote (2-of-3 judges). 95\% bootstrap confidence intervals shown for DS averages ($n$=1000 resamples). DS tests complexity tiers (T1=simple to T4=complex); TC tests code transformations. $^{a}$Top 3 models not statistically distinguishable (McNemar's $p$>0.05). $^{b}$Significantly worse than Claude ($p$<0.05). Inter-judge $\kappa$: DS 0.47--0.93, TC 0.04--0.77.}
\label{tab:detection_results}
\end{table*}

Table~\ref{tab:detection_results} presents detection performance across both benchmarks. On DS, Claude Opus 4.5 dominates with 86.5\% average TDR, achieving perfect detection on Tier 1 (simple contracts) and maintaining 70\%+ through Tier 3. Gemini 3 Pro follows at 73.9\%, while Grok 4 trails at 35.5\%.

The DS$\rightarrow$TC performance drop reveals memorization reliance. Claude and Gemini both experience $\sim$35pp drops (86.5\%$\rightarrow$50.9\% and 73.9\%$\rightarrow$38.5\%), suggesting heavy reliance on training data patterns. Models with smaller drops (Qwen: 20pp, Grok: 14pp) may rely less on memorization, though their baseline performance is also lower.

Among TC variants, Chameleon (domain shift) and ShapeShifter (code restructuring) cause the largest degradation, while Trojan variants show unexpected resistance---DeepSeek and Llama achieve their best TC scores (43.5\%) on this obfuscation type, suggesting different vulnerability pattern recognition strategies.

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/figure1_tc_obfuscation.png}
\caption{Temporal Contamination benchmark: Target Detection Rate across seven code transformation variants, ordered by obfuscation intensity. Light obfuscation (sanitization) preserves most detection capability, while heavy transformations (Chameleon domain shift, ShapeShifter restructuring) cause 30--50\% relative drops. Claude maintains highest performance across all variants; steep drops from MinSan$\rightarrow$Chameleon suggest memorization reliance.}
\label{fig:tc_obfuscation}
\end{figure*}

Figure~\ref{fig:tc_obfuscation} visualizes the TC performance trajectory across transformation intensity. The consistent ordering (Claude $>$ Gemini $>$ GPT-5.2) suggests robust relative rankings, but all models degrade substantially under heavy obfuscation---evidence that current LLMs partially rely on surface patterns rather than deep semantic understanding.

\subsection{Prompt Protocol Effects (Gold Standard)}

The Gold Standard (GS) benchmark uses 34 curated post-September 2025 samples to test prompt engineering effects without temporal contamination.

\begin{table}[!ht]
\centering
\small
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}lccccc|c@{}}
\toprule
\textbf{Model} & \textbf{Direct} & \textbf{Ctx} & \textbf{CoT} & \textbf{Nat} & \textbf{Adv} & \textbf{Avg [CI]} \\
\midrule
Claude & 11.8 & 26.5 & 26.5 & 20.6 & \textbf{41.2} & \textbf{25.3} \scriptsize{[18--33]} \\
Gemini & \textbf{17.6} & 20.6 & 17.6 & 26.5 & 32.4 & 22.9 \scriptsize{[16--30]} \\
GPT-5.2 & 5.9 & 11.8 & 14.7 & 29.4 & 29.4 & 18.2 \scriptsize{[12--25]} \\
Qwen & 0.0 & 5.9 & 14.7 & \textbf{32.4} & 17.6 & 14.1 \scriptsize{[8--21]} \\
DeepSeek & 0.0 & \textbf{20.6} & 8.8 & 17.6 & 17.6 & 12.9 \scriptsize{[7--20]} \\
Grok & 2.9 & 8.8 & 8.8 & 14.7 & 8.8 & 8.8 \scriptsize{[4--15]} \\
Llama & 2.9 & 0.0 & 8.8 & 2.9 & 0.0 & 2.9 \scriptsize{[0--7]} \\
\bottomrule
\end{tabular}
\caption{GS Target Detection Rate (\%) by prompt protocol ($n$=34 samples). 95\% bootstrap CIs shown for averages. Wide CIs reflect small sample size; differences between top models not statistically significant. Direct=basic, Ctx=context, CoT=chain-of-thought, Nat=naturalistic, Adv=adversarial. Inter-judge $\kappa$=0.31--1.00.}
\label{tab:gs_results}
\end{table}

Table~\ref{tab:gs_results} reveals striking prompt sensitivity. Claude benefits most from adversarial framing (+29.4pp over Direct), while Qwen shows dramatic improvement with naturalistic prompts (+32.4pp). Interestingly, CoT alone provides modest gains, but combining it with role-based framing (Nat/Adv) yields larger improvements.

Llama consistently underperforms ($\leq$8.8\% across all prompts), suggesting fundamental limitations rather than prompt sensitivity. Grok shows high inter-judge agreement ($\kappa$=0.76--1.00) but low TDR, indicating consistent but unsuccessful detection attempts.

\begin{figure}[!ht]
\centering
\includegraphics[width=\columnwidth]{figures/figure2_gs_protocol.png}
\caption{Gold Standard benchmark: Impact of prompt engineering strategies on detection performance. Adversarial framing (``You are a security auditor finding vulnerabilities'') provides the largest gains for Claude (+29pp) and Gemini (+15pp). Naturalistic framing helps Qwen (+32pp) but not others. Direct prompting yields lowest performance across all models.}
\label{fig:gs_protocol}
\end{figure}

Figure~\ref{fig:gs_protocol} reveals that prompt strategy significantly impacts detection. The adversarial framing advantage suggests models respond to role-based priming, while the naturalistic gains for Qwen may indicate different instruction-tuning approaches across model families.

\subsection{Transformation Robustness}

The DS$\rightarrow$TC performance degradation (Table~\ref{tab:detection_results}) reveals memorization patterns:

\textbf{Domain Shift (Chameleon).} Replacing blockchain terminology with medical vocabulary causes 30--50\% relative drops. Claude maintains 43.5\% (vs 86.5\% DS), while Qwen drops to 15.2\%.

\textbf{Code Restructuring (ShapeShifter).} Semantic-preserving transformations cause similar degradation. Llama suffers most (13.0\%), suggesting reliance on surface patterns.

\textbf{Trojan Variants.} Unexpectedly, hidden vulnerability variants show resistance---DeepSeek and Llama achieve their best TC scores (43.5\%), suggesting they detect patterns invisible to other models.

\subsection{Human Validation}

\paragraph{Human-Judge Agreement.} Two independent groups of human reviewers validated 1,000 stratified samples from our 3,500+ model-sample evaluations. When the LLM judge ensemble reached consensus (2+ judges agreeing), human reviewers concurred 70--90\% of the time, yielding Cohen's $\kappa \geq 0.68$ (``substantial'' agreement per Landis-Koch). Agreement was higher for consensus ``not found'' verdicts than ``found'' verdicts, suggesting judges are more reliable at ruling out false positives than confirming true vulnerabilities.

\paragraph{Inter-Human Agreement.} The two independent human reviewer groups achieved over 85\% agreement, establishing a reliability baseline. This inter-human variance contextualizes judge-human disagreements---some reflect genuine ambiguity in vulnerability assessment rather than judge error.

\paragraph{Inter-Judge Agreement.} The three LLM judges (GLM-4.7, Mistral Large, MIMO v2) achieved Fleiss' $\kappa$=0.78 (``substantial agreement'') on finding classification. Disagreements primarily involved \textsc{Partial\_Match} vs \textsc{Target\_Match} distinctions (67\% of disagreements) rather than valid/invalid classification ($\kappa$=0.89). Final classifications use majority voting; ties default to the more conservative judgment.

\subsection{Quality Metrics Analysis}

Beyond detection rate, we evaluate reasoning quality and finding reliability using the Security Understanding Index (SUI), a composite metric combining detection, reasoning, and precision.

\begin{table}[!ht]
\centering
\small
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}lcccccc|c@{}}
\toprule
\textbf{Model} & \textbf{SUI [CI]} & \textbf{Prec} & \textbf{RCIR} & \textbf{AVA} & \textbf{FSV} & \textbf{LGR} & \textbf{Hal.} \\
\midrule
Claude & \textbf{.76} \scriptsize{[.71--.81]} & 73.0 & 0.97 & 0.90 & 0.96 & \textbf{33.7} & \textbf{0.4} \\
GPT-5.2 & .74 \scriptsize{[.69--.79]} & \textbf{89.6} & \textbf{0.99} & \textbf{0.95} & \textbf{0.97} & 48.5 & 1.1 \\
Gemini & .74 \scriptsize{[.69--.79]} & 81.5 & \textbf{0.99} & 0.93 & 0.96 & 42.8 & 1.4 \\
Grok & .62 \scriptsize{[.56--.68]} & 74.5 & \textbf{0.99} & 0.94 & 0.94 & 57.3 & 1.3 \\
DeepSeek & .58 \scriptsize{[.52--.64]} & 41.0 & 0.96 & 0.87 & 0.93 & 52.8 & 2.1 \\
Qwen & .55 \scriptsize{[.49--.61]} & 41.0 & 0.92 & 0.80 & 0.89 & 56.6 & 0.6 \\
Llama & .48 \scriptsize{[.42--.54]} & 23.7 & 0.89 & 0.73 & 0.87 & 59.2 & 0.9 \\
\bottomrule
\end{tabular}
\caption{Quality metrics across DS+TC ($n$=422 samples). 95\% bootstrap CIs for SUI. SUI=Security Understanding Index (0.4$\times$TDR + 0.3$\times\bar{R}$ + 0.3$\times$Precision). Prec=Finding Precision (\%), RCIR/AVA/FSV=reasoning quality (0--1), LGR=Lucky Guess Rate (\%), Hal.=Hallucination Rate (\%). Claude and GPT-5.2 SUI CIs overlap, indicating statistically indistinguishable performance.}
\label{tab:quality_metrics}
\end{table}

Table~\ref{tab:quality_metrics} reveals nuanced performance differences. While GPT-5.2 achieves highest precision (89.6\%) and reasoning scores, Claude leads in SUI (0.76) due to superior TDR. The Lucky Guess Rate (LGR) provides critical insight: Claude's 33.7\% LGR indicates genuine understanding, while Llama's 59.2\% suggests pattern matching---correctly classifying code as vulnerable without identifying specific flaws.

\paragraph{SUI Sensitivity Analysis.} We tested five weight configurations (balanced, detection-default, quality-first, precision-first, detection-heavy). Rankings show high stability: Spearman's $\rho$=0.93--1.00 across configurations, with Claude and Gemini consistently in top 2. This stability validates SUI as a robust composite metric.

\paragraph{Statistical Significance.} McNemar's tests on DS reveal that top models are statistically indistinguishable: Claude vs Gemini ($p$=0.47), Claude vs GPT-5.2 ($p$=0.28), Gemini vs GPT-5.2 ($p$=0.51). Significant differences exist only between tier extremes: Claude vs Grok ($p$=0.002), Claude vs Qwen ($p$=0.02). This suggests the top three models have comparable detection capabilities, with differences potentially attributable to sampling variance.

\subsection{CodeAct Analysis}

Beyond detecting vulnerabilities, we analyze whether models truly understand root causes or merely match code patterns. TDR measures understanding---LLM judges evaluate reasoning quality. \colorbox{red!25}{\textsc{Root\_Cause}} matching measures pattern recognition---finding the security-critical code segments without necessarily explaining why. Using CodeAct annotations (Appendix~\ref{app:codeacts}), we compare these metrics on Trojan variants with injected \colorbox{violet!25}{\textsc{Decoy}} segments.

\begin{figure}[!ht]
\centering
\includegraphics[width=\columnwidth]{figures/figure3_tdr_vs_rootcause.png}
\caption{The Pattern Matching Paradox using CodeAct annotations ($n$=46 Trojan samples). X-axis: \colorbox{red!25}{\scriptsize\textsc{Root\_Cause}} match rate (pattern matching); Y-axis: TDR (reasoning quality, judge-evaluated). Points below the diagonal indicate models that locate \colorbox{red!25}{\scriptsize\textsc{Root\_Cause}} segments but provide poor explanations. Large gaps suggest pattern memorization rather than genuine understanding.}
\label{fig:tdr_vs_rootcause}
\end{figure}

Figure~\ref{fig:tdr_vs_rootcause} reveals a striking pattern matching paradox. Llama achieves the highest \colorbox{red!25}{\textsc{Root\_Cause}} match (60.9\%) but the lowest TDR (31.7\%)---it locates the security-critical segments but cannot articulate why they are vulnerable. This 29.2pp gap indicates pattern memorization rather than understanding.

The contamination index (Appendix~\ref{app:additional_results}) measures performance drop when \colorbox{violet!25}{\textsc{Decoy}} segments are added: high contamination indicates sensitivity to superficially suspicious code, while low contamination with high \colorbox{red!25}{\textsc{Root\_Cause}} match but low TDR indicates stable but superficial pattern matching.

Notably, all models achieve 100\% fix recognition on differential variants, not tagging any previous \colorbox{red!25}{\textsc{Root\_Cause}} that has become \colorbox{green!20}{\textsc{Benign}} as a new \colorbox{red!25}{\textsc{Root\_Cause}}. This asymmetry suggests models recognize \colorbox{green!20}{\textsc{Benign}} patterns more reliably than they understand \colorbox{red!25}{\textsc{Root\_Cause}} segments.
