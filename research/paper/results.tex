\section{Results}

We evaluate six frontier models on $n=58$ samples from BlockBench: 10 GS, 20 TC, 28 DS.

\subsection{Overall Performance}

\begin{table*}[!ht]
\centering
\small
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Model} & \textbf{TDR} & \textbf{SUI} & \textbf{Acc} & \textbf{RCIR} & \textbf{AVA} & \textbf{FSV} & \textbf{Findings} \\
\midrule
Gemini 3 Pro & \textbf{57.6} & 0.734 & \textbf{93.9} & 0.97 & 0.97 & 0.95 & 2.6 \\
GPT-5.2 & 55.9 & \textbf{0.746} & 75.0 & 0.97 & 0.98 & \textbf{0.97} & 2.4 \\
Claude Opus 4.5 & 52.9 & 0.703 & 83.8 & 0.98 & 0.99 & 0.97 & 3.5 \\
Grok 4 & 44.1 & 0.677 & 69.1 & \textbf{0.98} & \textbf{1.00} & \textbf{0.97} & 2.1 \\
DeepSeek v3.2 & 38.2 & 0.599 & 82.4 & 0.91 & 0.92 & 0.86 & 3.0 \\
Llama 3.1 405B & 17.9 & 0.393 & \textbf{88.1} & 0.88 & 0.90 & 0.83 & 2.0 \\
\bottomrule
\end{tabular}
\caption{Overall performance ranked by Target Detection Rate. Best values bold.}
\label{tab:overall_results}
\end{table*}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\columnwidth]{figures/tdr_comparison.png}
\caption{Target Detection Rate across all models. Best performer achieves 58\% detection, while highest accuracy (88\%) corresponds to lowest TDR (18\%).}
\label{fig:tdr_comparison}
\end{figure}

Table~\ref{tab:overall_results} and Figure~\ref{fig:tdr_comparison} show aggregate performance by TDR. Gemini 3 Pro achieves highest detection (58\%), followed by GPT-5.2 (56\%) and Claude Opus 4.5 (53\%). Combining detection, reasoning, and precision into SUI, GPT-5.2 ranks first (0.746) on finding precision (77\%).

Llama 3.1 405B exhibits severe accuracy-TDR gap: 88\% accuracy yet 18\% TDR, classifying samples as vulnerable without identifying specific flaws. This 70pp discrepancy shows binary classification inadequately measures security understanding. Models achieving target detection show strong reasoning (RCIR/AVA/FSV $\geq$0.95).

\subsection{Gold Standard Performance}

Gold Standard samples from post-September 2025 audits ensure zero temporal contamination. Performance drops substantially: Claude Opus 4.5 leads (20\% TDR), followed by Gemini 3 Pro (11\%), GPT-5.2 (10\%), Grok 4 (10\%). DeepSeek v3.2 and Llama detect zero targets. Models experience 34-50pp drops from overall to Gold Standard.

\subsection{Transformation Robustness}

\textbf{Sanitization.} Neutralizing security-suggestive identifiers causes variable degradation. GPT-5.2 and DeepSeek v3.2 maintain performance, while Grok 4 drops 40pp, exposing varying lexical reliance.

\textbf{Domain Shift.} Replacing blockchain terminology with medical vocabulary shows mixed impact (20-60\% TDR). GPT-5.2 maintains 60\% detection while others degrade 20-50\%.

\textbf{Prompt Framing.} Performance varies across direct, adversarial, and naturalistic prompts. Gemini 3 Pro and GPT-5.2 show robustness (18-21pp drops), while Claude Opus 4.5 and DeepSeek v3.2 degrade more (21-39pp). Llama exhibits inconsistent behavior (adversarial: 0\%, naturalistic: 25\%).

\subsection{Human Validation}

\paragraph{Expert-Judge Agreement.} Two security professionals (5+ years smart contract auditing experience) independently validated a stratified sample of 31 contracts (10\% of dataset, balanced across difficulty tiers and vulnerability types), producing 116 expert-judge comparisons. Expert-judge agreement reached 92\% ($\kappa$=0.84, ``almost perfect'' per Landis-Koch), with Spearman's $\rho$=0.85 for quality scores (p<0.0001). The LLM judge achieved perfect recall (1.00) with 84\% precision (F1=0.91), confirming all expert-identified vulnerabilities while flagging 9 additional edge cases reviewed as valid.

\paragraph{Inter-Judge Agreement.} The three LLM judges (GLM-4.7, Mistral Large, MIMO v2) achieved Fleiss' $\kappa$=0.78 (``substantial agreement'') on finding classification across 2,030 judgments. Disagreements primarily involved \textsc{Partial\_Match} vs \textsc{Target\_Match} distinctions (67\% of disagreements) rather than valid/invalid classification ($\kappa$=0.89). For quality scores, intraclass correlation ICC(2,3)=0.82 indicates strong consistency. Final classifications use majority voting; ties default to the more conservative judgment.
