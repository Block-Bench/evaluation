\section{Discussion}

\textbf{Understanding versus Memorization.}
The pattern matching paradox (Figure~\ref{fig:tdr_vs_rootcause}) reveals heterogeneous robustness across models. Llama achieves the highest \colorbox{red!25}{\textsc{Root\_Cause}} match (60.9\%) but the lowest TDR (31.7\%), finding vulnerable code through pattern recognition without understanding why it is vulnerable. The contamination index further distinguishes genuine understanding from memorization: Claude's 36.6\% contamination indicates sensitivity to \colorbox{violet!25}{\textsc{Decoy}} distractions, while Llama's minimal 6.7\% drop paradoxically reveals stable but superficial pattern matching. This heterogeneity suggests current training methods produce inconsistent abstraction capabilities across architectures.

\textbf{Measurement Inadequacy.}
The \colorbox{red!25}{\textsc{Root\_Cause}} match versus TDR gap exposes fundamental metric limitations. A model achieving high line-level matching yet low TDR correctly locates vulnerable code without articulating exploit mechanics. For security practitioners requiring actionable findings, pattern matching provides insufficient value. Effective evaluation must measure both precise vulnerability localization and causal reasoning, not merely code segment identification.

\textbf{Practical Implications.}
Current frontier models cannot serve as autonomous auditors. Best DS performance reaches 86.5\% (Claude), but degrades to 50.9\% on TC variants and 25.3\% on Gold Standard post-cutoff samples. However, complementary strengths suggest ensemble potential: Claude delivers highest TDR and explanation quality, GPT-5.2 provides highest precision (89.6\%) and reasoning scores, while models respond differently to prompt engineering (adversarial framing boosts Claude by 29pp on GS). Workflows positioning LLMs as assistive tools with mandatory expert review align capabilities with current limitations.
