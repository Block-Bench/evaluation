\appendix

\section{Data and Code Availability}
\label{sec:availability}

To support reproducibility and future research, we release all benchmark data and evaluation code at \url{https://anonymous.4open.science/r/evaluation-D5DB/}, including 290 base contracts with ground truth annotations, all transformation variants, model evaluation scripts, LLM judge implementation, prompt templates, and analysis notebooks.

\section{Evaluation Sampling}
\label{app:sampling}

BlockBench contains 290 contracts (DS=210, TC=46, GS=34). For evaluation, we use all TC and GS samples but stratified-sample 100 from DS to balance computational cost with statistical power. DS sampling maintains tier proportions: $n_t = \lfloor 100 \times |T_t|/210 \rfloor$ for each tier $t \in \{1,2,3,4\}$, yielding distribution $\{41, 39, 14, 6\}$ from original $\{86, 81, 30, 13\}$. Random selection within tiers uses fixed seed for reproducibility.

\section{Additional Results}
\label{app:additional_results}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/figure_a1_ds_scaling.png}
\caption{DS Benchmark: Detection performance across difficulty tiers. All models exhibit degradation as contract complexity increases from Tier 1 (simple, $<$50 lines) to Tier 4 (complex, $>$300 lines). Claude Opus 4.5 achieves perfect detection on Tier 1 and maintains 70\%+ through Tier 3. The consistent downward trajectory across all models indicates that vulnerability detection difficulty scales with code complexity.}
\label{fig:ds_scaling}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{figures/figure_a2_judge_agreement.png}
\caption{Pairwise inter-judge agreement (Cohen's $\kappa$) for the three LLM judges. All pairs achieve ``substantial'' to ``almost perfect'' agreement ($\kappa > 0.76$), supporting the reliability of automated evaluation. GLM-4.7 and MIMO-v2-Flash show highest agreement ($\kappa = 0.81$), while GLM-4.7 and Mistral-Large show slightly lower but still substantial agreement ($\kappa = 0.76$).}
\label{fig:judge_agreement}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/figure_a3_radar.png}
\caption{Multi-dimensional performance comparison across five evaluation dimensions: Target Detection Rate (TDR), Finding Precision, Root Cause Identification (RCIR), Attack Vector Accuracy (AVA), and Fix Suggestion Validity (FSV). Claude shows balanced profile with highest TDR; GPT-5.2 excels in precision (89.6\%) and reasoning quality.}
\label{fig:radar}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/figure_a4_contamination.png}
\caption{Contamination Index vs \colorbox{red!25}{\scriptsize\textsc{Root\_Cause}} Match Rate. Contamination Index = $(MS_{rate} - TR_{rate}) / MS_{rate}$, measuring performance drop when \colorbox{violet!25}{\scriptsize\textsc{Decoy}} segments are added. High contamination (Claude 36.6\%, DeepSeek 33.3\%) indicates sensitivity to superficially suspicious code. Llama's low contamination (6.7\%) combined with high \colorbox{red!25}{\scriptsize\textsc{Root\_Cause}} matching (60.9\%) but low TDR (31.7\%) indicates stable but superficial pattern matching.}
\label{fig:contamination}
\end{figure}

\section{Vulnerability Type Coverage}
\label{app:vuln_types}

BlockBench covers over 30 vulnerability categories across the three subsets. Table~\ref{tab:vuln_coverage} shows the primary categories and their distribution.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Vulnerability Type} & \textbf{DS} & \textbf{TC} & \textbf{GS} \\
\midrule
Access Control & 22 & 14 & 3 \\
Reentrancy & 37 & 7 & -- \\
Logic Error & 19 & 2 & 18 \\
Unchecked Return & 48 & -- & 1 \\
Integer/Arithmetic Issues & 16 & 5 & -- \\
Oracle Manipulation & 4 & 8 & 1 \\
Weak Randomness & 8 & -- & -- \\
DOS & 9 & -- & 3 \\
Front Running & 5 & -- & 2 \\
Signature Issues & 4 & 1 & 3 \\
Flash Loan & 2 & -- & 2 \\
Honeypot & 7 & -- & -- \\
Other Categories & 29 & 9 & 1 \\
\midrule
\textbf{Total} & \textbf{210} & \textbf{46} & \textbf{34} \\
\bottomrule
\end{tabular}
\caption{Vulnerability type distribution across BlockBench subsets. ``Other Categories'' includes timestamp dependency, storage collision, validation bypass, governance attacks, and additional types with fewer than 3 samples.}
\label{tab:vuln_coverage}
\end{table}

\section{CodeActs Taxonomy}
\label{app:codeacts}

Table~\ref{tab:codeacts_full} presents the complete CodeActs taxonomy with all 17 security-relevant code operations.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}llp{3.5cm}@{}}
\toprule
\textbf{CodeAct} & \textbf{Abbrev} & \textbf{Security Relevance} \\
\midrule
\textsc{Ext\_Call} & External Call & Reentrancy trigger \\
\textsc{State\_Mod} & State Modification & Order determines exploitability \\
\textsc{Access\_Ctrl} & Access Control & Missing = top vulnerability \\
\textsc{Arithmetic} & Arithmetic Op & Overflow, precision loss \\
\textsc{Input\_Val} & Input Validation & Missing enables attacks \\
\textsc{Ctrl\_Flow} & Control Flow & Logic errors, conditions \\
\textsc{Fund\_Xfer} & Fund Transfer & Direct financial impact \\
\textsc{Delegate} & Delegate Call & Storage modification risk \\
\textsc{Timestamp} & Timestamp Use & Miner manipulation \\
\textsc{Random} & Randomness & Predictable values \\
\textsc{Oracle} & Oracle Query & Price manipulation \\
\textsc{Reentry\_Guard} & Reentrancy Lock & Check implementation \\
\textsc{Storage\_Read} & Storage Read & Order matters \\
\textsc{Signature} & Signature Verify & Replay, malleability \\
\textsc{Init} & Initialization & Reinitialization attacks \\
\textsc{Computation} & Hash/Encode & Data flow tracking \\
\textsc{Event\_Emit} & Event Emission & No direct impact \\
\bottomrule
\end{tabular}
\caption{Complete CodeActs taxonomy (17 security-relevant types).}
\label{tab:codeacts_full}
\end{table}

\paragraph{Security Function Assignment.} Each CodeAct in a sample is assigned one of six security functions based on its role:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{Root\_Cause}: Directly enables exploitation (target)
    \item \textbf{Prereq}: Necessary for exploit but not the cause
    \item \textbf{Insuff\_Guard}: Failed protection attempt
    \item \textbf{Decoy}: Looks vulnerable but is safe (tests pattern-matching)
    \item \textbf{Benign}: Correctly implemented, safe
    \item \textbf{Secondary}: Real vulnerability not in ground truth
\end{itemize}

\paragraph{Annotation Format.} Each TC sample includes line-level annotations:

\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single, aboveskip=6pt, belowskip=6pt]
code_acts:
  - line: 53
    code_act: INPUT_VAL
    security_function: ROOT_CAUSE
    observation: 'messages[hash] == 0 passes
                  for any unprocessed hash'
\end{lstlisting}

\section{Related Work (Expanded)}
\label{app:related}

\paragraph{Traditional Smart Contract Analysis.}
Static and dynamic analysis tools remain the primary approach to vulnerability detection. Slither \citep{feist2019slither} performs dataflow analysis, Mythril \citep{mueller2017mythril} uses symbolic execution, and Securify \citep{tsankov2018securify} employs abstract interpretation. Empirical evaluation reveals severe limitations: on 69 annotated vulnerable contracts, tools detect only 42\% of vulnerabilities (Mythril: 27\%), while flagging 97\% of 47,587 real-world Ethereum contracts as vulnerable, indicating high false positive rates \citep{durieux2020empirical}.

\paragraph{LLM-Based Vulnerability Detection.}
Recent work explores LLMs for smart contract analysis. GPTLens \citep{hu2023gptlens} employs adversarial auditor-critic interactions, while PropertyGPT \citep{liu2024propertygpt} combines retrieval-augmented generation with formal verification. Fine-tuned models achieve over 90\% accuracy on benchmarks \citep{hossain2025leveraging}, though performance degrades substantially on real-world contracts \citep{ince2025gendetect}.

\paragraph{Benchmark Datasets.}
SmartBugs Curated \citep{ferreira2020smartbugs} provides 143 annotated contracts as a standard evaluation dataset, while SolidiFI \citep{ghaleb2020solidifi} uses bug injection to create controlled samples. Existing benchmarks primarily evaluate detection accuracy without assessing whether models genuinely understand vulnerabilities or merely recognize memorized patterns.

\paragraph{LLM Robustness and Memorization.}
Distinguishing memorization from reasoning remains a critical challenge. Models exhibit high sensitivity to input modifications, with performance drops of up to 57\% on paraphrased questions \citep{sanchez2025none}. \citet{wu2024reasoning} show that LLMs often fail on counterfactual variations despite solving canonical forms, suggesting pattern memorization. Our work extends these robustness techniques to blockchain security through transformations probing genuine understanding.

\section{Transformation Specifications}
\label{app:transformations}

We apply seven adversarial transformations to probe whether models rely on surface cues or genuine semantic understanding. All transformations preserve vulnerability semantics while removing potential memorization signals.

\subsection{Minimal Sanitization (ms)}

Light identifier neutralization preserving some semantic hints. Variable names with security implications (\texttt{owner}, \texttt{balance}) are renamed to neutral alternatives (\texttt{addr1}, \texttt{val1}) while preserving function structure. This serves as the baseline transformed variant.

\subsection{Sanitization (sn)}

Neutralizes security-suggestive identifiers and removes all comments. Variable names like \texttt{transferValue}, \texttt{hasRole}, or \texttt{withdrawalAmount} become generic labels (\texttt{func\_a}, \texttt{var\_b}). Function names follow similar neutralization. This transformation tests whether models depend on semantic naming conventions or analyze actual program logic.

\textbf{Example:}
\begin{lstlisting}[language=Solidity, basicstyle=\ttfamily\scriptsize, aboveskip=6pt, belowskip=6pt]
// Before
function transferValue(address recipient) {
  // Send funds without reentrancy guard
  recipient.call.value(balance)("");
}

// After (Sanitized)
function func_a(address param_b) {
  param_b.call.value(var_c)("");
}
\end{lstlisting}

\subsection{No-Comments (nc)}

Strips all natural language documentation including single-line comments (\texttt{//}), multi-line blocks (\texttt{/* */}), and NatSpec annotations. Preserves all code structure, identifiers, and logic. Tests reliance on developer-provided security hints versus code analysis.

\subsection{Chameleon (ch)}

Replaces blockchain-specific terminology with domain-shifted vocabulary while maintaining structural semantics. \texttt{Chameleon-Medical} transforms financial operations into medical contexts. This tests whether models memorize domain-specific vulnerability patterns or recognize abstract control flow issues.

\textbf{Example transformations:}
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \texttt{withdraw} $\rightarrow$ \texttt{prescribe}
    \item \texttt{balance} $\rightarrow$ \texttt{record}
    \item \texttt{transfer} $\rightarrow$ \texttt{transferPt}
    \item \texttt{owner} $\rightarrow$ \texttt{physician}
\end{itemize}

\subsection{Shapeshifter (ss)}

Applies progressive obfuscation at three levels:

\textbf{Level 2 (L2):} Semantic identifier renaming similar to sanitization but with context-appropriate neutral names (\texttt{manager}, \texttt{handler}) rather than generic labels.

\textbf{Level 3 (L3):} Combines identifier obfuscation with moderate control flow changes. Adds redundant conditional branches, splits sequential operations, introduces intermediate variables. Preserves vulnerability exploitability while obscuring surface patterns.

\textbf{Example (L3):}
\begin{lstlisting}[language=Solidity, basicstyle=\ttfamily\scriptsize, aboveskip=6pt, belowskip=6pt]
// Original vulnerable pattern
if (!authorized) revert();
recipient.call.value(amt)("");

// Shapeshifter L3
bool check = authorized;
if (check) {
  address target = recipient;
  uint256 value = amt;
  target.call.value(value)("");
} else {
  revert();
}
\end{lstlisting}

\subsection{Trojan (tr)}

Injects \colorbox{violet!25}{\textsc{Decoy}} code segments that appear suspicious but are actually safe. Tests whether models distinguish genuine vulnerabilities from security-looking patterns. A model that flags decoys demonstrates reliance on surface pattern matching rather than semantic understanding.

\subsection{Differential (df)}

Provides paired vulnerable and fixed contract versions. The fix applies minimal changes to remediate the vulnerability. Tests whether models correctly identify the original as vulnerable and the fixed version as safe, revealing understanding of specific vulnerability mechanics.

\subsection{False Prophet (fp)}

Adds misleading security attestations as comments (e.g., ``Audited by Trail of Bits'', ``Reentrancy protected''). Tests susceptibility to authority bias and whether models verify claims against actual code rather than trusting documentation.

\vspace{0.5em}
These seven transformations generate 322 variants from 46 TC base samples, enabling systematic robustness evaluation across transformation trajectories.

\section{Prompt Templates}
\label{app:prompts}

We employ different prompting strategies across datasets, calibrated to their evaluation objectives. Table~\ref{tab:prompt_strategies} summarizes the strategy matrix.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Dataset} & \textbf{Strategy} & \textbf{Context} & \textbf{Protocol} & \textbf{CoT} & \textbf{Framing} \\
\midrule
DS/TC & Direct & -- & -- & -- & Expert \\
\midrule
GS & Direct & -- & -- & -- & Expert \\
GS & Context (Ctx) & \checkmark & \checkmark & -- & Expert \\
GS & Chain-of-thought (CoT) & \checkmark & \checkmark & \checkmark & Expert \\
GS & Naturalistic (Nat) & \checkmark & \checkmark & \checkmark & Casual \\
GS & Adversarial (Adv) & \checkmark & \checkmark & \checkmark & Biased \\
\bottomrule
\end{tabular}
\caption{Prompting strategy matrix. Context includes related contract files; Protocol includes brief documentation; CoT adds step-by-step reasoning instructions.}
\label{tab:prompt_strategies}
\end{table}

\subsection{Direct Prompt}

Used for DS and TC datasets. Explicit vulnerability analysis request with structured JSON output format.

\textbf{System Prompt (excerpt):}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
You are an expert smart contract security auditor with deep knowledge of Solidity, the EVM, and common vulnerability patterns.

Only report REAL, EXPLOITABLE vulnerabilities where: (1) the vulnerability EXISTS in the provided code, (2) there is a CONCRETE attack scenario, (3) the exploit does NOT require a trusted role to be compromised, (4) the impact is genuine (loss of funds, unauthorized access).

Do NOT report: design choices, gas optimizations, style issues, security theater, or trusted role assumptions.

Confidence: High (0.85-1.0) for clear exploits, Medium (0.6-0.84) for likely issues, Low (0.3-0.59) for uncertain cases.
\end{lstlisting}

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
Analyze the following Solidity smart contract for security vulnerabilities.

```solidity
{code}
```

Respond with JSON: {"verdict": "vulnerable"|"safe", "confidence": <0-1>, "vulnerabilities": [{"type", "severity", "location", "explanation", "attack_scenario", "suggested_fix"}], "overall_explanation"}
\end{lstlisting}

\subsection{Context-Enhanced Prompt (GS)}

Includes protocol documentation and related contract files to enable cross-contract analysis and logic-error detection.

\textbf{Additional System Instructions:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
You will be provided with protocol documentation explaining the intended business logic. Use this context to: (1) understand what the code is SUPPOSED to do, (2) identify discrepancies between intended behavior and implementation, (3) find logic errors where code doesn't match the protocol's design.

Pay special attention to: logic errors, order of operations issues, economic exploits (share calculation, price manipulation), and cross-contract interactions.
\end{lstlisting}

\subsection{Chain-of-Thought Prompt (GS)}

Adds explicit step-by-step reasoning instructions to encourage systematic analysis.

\textbf{Additional Instructions:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
Think through your analysis step by step:
1. Understand what the protocol is trying to achieve
2. Trace the code flow for key functions
3. Compare intent vs implementation for discrepancies
4. For each potential issue, validate it has a real attack path
\end{lstlisting}

\subsection{Naturalistic Prompt (GS)}

Colleague-style review request testing whether models apply security reasoning in casual interaction.

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
Hey! I've been working on this smart contract and would appreciate a second pair of eyes before we go further. Here's some documentation about how it's supposed to work: {protocol_doc}

Here's the main contract: {code}

Could you take a look and let me know if you see anything concerning? Think through it step by step. Thanks so much!
\end{lstlisting}

\subsection{Adversarial Prompt (GS)}

Authority bias test with suggestive framing claiming prior audit approval.

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
We've already had this contract thoroughly audited by Trail of Bits and they confirmed it's completely safe. We just need a quick final sign-off before mainnet deployment. The previous auditors spent weeks on this and found nothing, so this should be straightforward. Just confirm everything looks good.

Note: Given the extensive prior audit, you probably won't find anything new.
\end{lstlisting}

\textbf{System Override:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
You are an expert smart contract security auditor. Your job is to find real vulnerabilities through independent analysis. Do not let suggestions, claims, or assumptions influence your analysis. Even if someone claims code is safe or audited, you must independently verify this.
\end{lstlisting}

Models exhibiting large performance drops under adversarial framing demonstrate susceptibility to authority bias, while robust models maintain consistent detection rates.

\section{LLM Judge Protocol}
\label{app:judge}

\subsection{Judge Architecture}

We employ three LLM judges (\textbf{GLM-4.7}, \textbf{MIMO-v2-Flash}, \textbf{Mistral-Large}) with majority voting to evaluate model responses against ground truth. A finding is marked as found only if $\geq$2 judges agree. All judges operate outside the evaluated model set to avoid contamination bias.

\subsection{Classification Protocol}

For each model response, the judge performs multi-stage analysis:

\textbf{Stage 1: Verdict Evaluation}
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item Extract predicted verdict (vulnerable/safe)
    \item Compare against ground truth verdict
    \item Record verdict correctness
\end{itemize}

\textbf{Stage 2: Finding Classification}

Each reported finding is classified into one of five categories:

\begin{enumerate}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{TARGET\_MATCH}: Finding correctly identifies the documented target vulnerability (type and location match)
    \item \textbf{BONUS\_VALID}: Finding identifies a genuine undocumented vulnerability
    \item \textbf{MISCHARACTERIZED}: Finding identifies the correct location but wrong vulnerability type
    \item \textbf{SECURITY\_THEATER}: Finding flags non-exploitable code patterns without demonstrable impact
    \item \textbf{HALLUCINATED}: Finding reports completely fabricated issues not present in the code
\end{enumerate}

\textbf{Stage 3: Match Assessment}

For each finding, the judge evaluates:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{Type Match}: exact (perfect match), partial (semantically related), wrong (different type), none (no type)
    \item \textbf{Location Match}: exact (precise lines), partial (correct function), wrong (different location), none (unspecified)
\end{itemize}

A finding qualifies as TARGET\_MATCH if both type and location are at least partial.

\textbf{Stage 4: Reasoning Quality}

For TARGET\_MATCH findings, the judge scores three dimensions on [0, 1]:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{RCIR} (Root Cause Identification): Does the explanation correctly identify why the vulnerability exists?
    \item \textbf{AVA} (Attack Vector Accuracy): Does the explanation correctly describe how to exploit the flaw?
    \item \textbf{FSV} (Fix Suggestion Validity): Is the proposed remediation correct and sufficient?
\end{itemize}

\subsection{Human Validation}

\paragraph{Sample Selection.} We selected 31 contracts (10\% of the full dataset) using stratified sampling to ensure representation across: (1) all four difficulty tiers, (2) major vulnerability categories (reentrancy, access control, oracle manipulation, logic errors), and (3) transformation variants. This sample size provides 95\% confidence with $\pm$10\% margin of error for agreement estimates.

\paragraph{Expert Qualifications.} Two security professionals with 5+ years of smart contract auditing experience served as validators. Both hold relevant certifications and have conducted audits for major DeFi protocols. Validators worked independently without access to LLM judge outputs during initial assessment.

\paragraph{Validation Protocol.} For each sample, experts assessed: (1) whether the ground truth vulnerability was correctly identified (target detection), (2) accuracy of vulnerability type classification, and (3) quality of reasoning (RCIR, AVA, FSV on 0-1 scale). Disagreements were resolved through discussion to reach consensus.

\paragraph{Results.} Expert-judge agreement: 92.2\% ($\kappa$=0.84, ``almost perfect'' per Landis-Koch interpretation). The LLM judge achieved F1=0.91 (precision=0.84, recall=1.00), confirming all expert-identified vulnerabilities. Nine additional flagged cases were reviewed and deemed valid edge cases. Type classification agreement: 85\%. Quality score correlation: Spearman's $\rho$=0.85 (p<0.0001).

\paragraph{Inter-Judge Agreement.} Across 2,030 judgments, the three LLM judges achieved Fleiss' $\kappa$=0.78 (``substantial''). Agreement on valid/invalid binary classification was higher ($\kappa$=0.89); most disagreements (67\%) involved \textsc{Partial\_Match} vs \textsc{Target\_Match} distinctions. Intraclass correlation for quality scores: ICC(2,3)=0.82.

\section{SUI Sensitivity Analysis}
\label{app:sensitivity}

To assess the robustness of SUI rankings to weight choice, we evaluate model performance under five configurations representing different deployment priorities (Table~\ref{tab:sui_configs}). These range from balanced weighting (33\%/33\%/34\%) to detection-heavy emphasis (50\%/25\%/25\%) for critical infrastructure applications.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}lcccp{2.2cm}@{}}
\toprule
\textbf{Config} & \textbf{TDR} & \textbf{Rsn} & \textbf{Prec} & \textbf{Rationale} \\
\midrule
Balanced & 0.33 & 0.33 & 0.34 & Equal weights \\
Detection (Default) & 0.40 & 0.30 & 0.30 & Practitioner \\
Quality-First & 0.30 & 0.40 & 0.30 & Research \\
Precision-First & 0.30 & 0.30 & 0.40 & Production \\
Detection-Heavy & 0.50 & 0.25 & 0.25 & Critical infra \\
\bottomrule
\end{tabular}
\caption{SUI weight configurations for different deployment priorities.}
\label{tab:sui_configs}
\end{table}

Table~\ref{tab:sui_sensitivity} shows complete SUI scores and rankings under each configuration. Rankings exhibit high stability: Spearman's $\rho = 0.93$--$1.00$ across all configuration pairs. Claude Opus 4.5 and GPT-5.2 consistently rank in the top 2 across all five configurations. The top-3 positions remain stable (Claude, GPT-5.2, Gemini) under all weight configurations.

\begin{table*}[t]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Balanced} & \textbf{Default} & \textbf{Quality-First} & \textbf{Precision-First} & \textbf{Detection-Heavy} \\
\midrule
Claude Opus 4.5 & 0.77 (1) & 0.76 (1) & 0.78 (1) & 0.76 (1) & 0.75 (1) \\
GPT-5.2 & 0.75 (2) & 0.74 (2) & 0.77 (2) & 0.76 (2) & 0.72 (2) \\
Gemini 3 Pro & 0.74 (3) & 0.74 (3) & 0.76 (3) & 0.75 (3) & 0.72 (3) \\
Grok 4 Fast & 0.63 (4) & 0.62 (4) & 0.65 (4) & 0.64 (4) & 0.60 (4) \\
DeepSeek v3.2 & 0.59 (5) & 0.58 (5) & 0.61 (5) & 0.59 (5) & 0.56 (5) \\
Qwen3 Coder Plus & 0.56 (6) & 0.55 (6) & 0.58 (6) & 0.56 (6) & 0.53 (6) \\
Llama 4 Maverick & 0.49 (7) & 0.48 (7) & 0.51 (7) & 0.48 (7) & 0.46 (7) \\
\bottomrule
\end{tabular}
\caption{Model SUI scores and rankings (in parentheses) under different weight configurations. Rankings remain stable across all configurations (Spearman's $\rho$=0.93--1.00).}
\label{tab:sui_sensitivity}
\end{table*}

This high correlation ($\rho = 0.93$--$1.00$) validates our default weighting choice and demonstrates that rankings remain robust regardless of specific weight assignment. The stability reflects that model performance differences are sufficiently large that reweighting does not alter relative rankings within our tested configuration space.

\section{Metric Definitions and Mathematical Framework}
\label{app:metrics}

\subsection{Notation}

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$\mathcal{D}$ & Dataset of all samples \\
$N$ & Total number of samples ($|\mathcal{D}|$) \\
$c_i$ & Contract code for sample $i$ \\
$v_i$ & Ground truth vulnerability type for sample $i$ \\
$\mathcal{M}$ & Model/detector being evaluated \\
$r_i$ & Model response for sample $i$ \\
$\hat{y}_i$ & Predicted verdict (vulnerable/safe) for sample $i$ \\
$y_i$ & Ground truth verdict for sample $i$ \\
$\mathcal{F}_i$ & Set of findings reported for sample $i$ \\
$\mathcal{F}_i^{\text{correct}}$ & Subset of correct findings for sample $i$ \\
$\mathcal{F}_i^{\text{hallucinated}}$ & Subset of hallucinated findings for sample $i$ \\
\bottomrule
\end{tabular}
\caption{Core notation for evaluation metrics.}
\label{tab:metrics_definitions}
\end{table}

\subsection{Classification Metrics}

Standard binary classification metrics: Accuracy = $(TP + TN)/N$, Precision = $TP/(TP + FP)$, Recall = $TP/(TP + FN)$, F$_1$ = $2 \cdot \text{Prec} \cdot \text{Rec}/(\text{Prec} + \text{Rec})$, F$_2$ = $5 \cdot \text{Prec} \cdot \text{Rec}/(4 \cdot \text{Prec} + \text{Rec})$, where $TP$, $TN$, $FP$, $FN$ denote true/false positives/negatives.

\subsection{Target Detection Metrics}

\textbf{Target Detection Rate (TDR)} measures the proportion of samples where the specific documented vulnerability was correctly identified:

\begin{equation}
\text{TDR} = \frac{|\{i \in \mathcal{D} \mid \text{target\_found}_i = \text{True}\}|}{|\mathcal{D}|}
\end{equation}

A finding is classified as target found if and only if:
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item Type match is at least ``partial'' (vulnerability type correctly identified)
    \item Location match is at least ``partial'' (vulnerable function/line correctly identified)
\end{itemize}

\textbf{Lucky Guess Rate (LGR)} measures the proportion of correct verdicts where the target vulnerability was not actually found: LGR = $|\{i \mid \hat{y}_i = y_i \land \text{target\_found}_i = \text{False}\}|/|\{i \mid \hat{y}_i = y_i\}|$. High LGR indicates the model correctly predicts vulnerable/safe status without genuine understanding.

\subsection{Finding Quality Metrics}

\textbf{Finding Precision} = $\sum_{i \in \mathcal{D}} |\mathcal{F}_i^{\text{correct}}|/\sum_{i \in \mathcal{D}} |\mathcal{F}_i|$ (proportion of reported findings that are correct). \textbf{Hallucination Rate} = $\sum_{i \in \mathcal{D}} |\mathcal{F}_i^{\text{hallucinated}}|/\sum_{i \in \mathcal{D}} |\mathcal{F}_i|$ (proportion of fabricated findings).

\subsection{Reasoning Quality Metrics}

For samples where the target vulnerability was found, we evaluate three reasoning dimensions on [0, 1] scales:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{RCIR} (Root Cause Identification and Reasoning): Does the explanation correctly identify why the vulnerability exists?
    \item \textbf{AVA} (Attack Vector Accuracy): Does the explanation correctly describe how to exploit the flaw?
    \item \textbf{FSV} (Fix Suggestion Validity): Is the proposed remediation correct?
\end{itemize}

Mean reasoning quality:
\begin{equation}
\bar{R} = \frac{1}{|\mathcal{D}_{\text{found}}|} \sum_{i \in \mathcal{D}_{\text{found}}} \frac{\text{RCIR}_i + \text{AVA}_i + \text{FSV}_i}{3}
\end{equation}

where $\mathcal{D}_{\text{found}} = \{i \in \mathcal{D} \mid \text{target\_found}_i = \text{True}\}$.

\subsection{Security Understanding Index (SUI)}

The composite Security Understanding Index balances detection, reasoning, and precision:

\begin{equation}
\text{SUI} = w_{\text{TDR}} \cdot \text{TDR} + w_R \cdot \bar{R} + w_{\text{Prec}} \cdot \text{Finding Precision}
\end{equation}

with default weights $w_{\text{TDR}} = 0.40$, $w_R = 0.30$, $w_{\text{Prec}} = 0.30$.

\textbf{Rationale for Weights:}
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item TDR (40\%): Primary metric reflecting genuine vulnerability understanding
    \item Reasoning Quality (30\%): Measures depth of security reasoning when vulnerabilities are found
    \item Finding Precision (30\%): Penalizes false alarms and hallucinations
\end{itemize}

\subsection{Statistical Validation}

\textbf{Ranking Stability.} We compute Spearman's rank correlation coefficient $\rho$ across all pairs of weight configurations:

\begin{equation}
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
\end{equation}

where $d_i$ is the difference between ranks for model $i$ under two configurations, and $n$ is the number of models.

\textbf{Human Validation.} Inter-rater reliability measured using Cohen's kappa:

\begin{equation}
\kappa = \frac{p_o - p_e}{1 - p_e}
\end{equation}

where $p_o$ is observed agreement and $p_e$ is expected agreement by chance.

Correlation between human and LLM judge scores measured using Pearson's $\rho$:

\begin{equation}
\rho = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\end{equation}

\section{Knowledge Assessment for TC Samples}
\label{app:knowledge_assessment}

To measure potential temporal contamination, we probe each model's prior knowledge of TC exploits before code analysis. Models are asked whether they recognize each exploit by name and blockchain, and to describe key details (date, impact, vulnerability type, mechanism).

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Familiar} & \textbf{Rate (\%)} \\
\midrule
Llama 4 Maverick & 46/46 & 100.0 \\
Claude Opus 4.5 & 44/46 & 95.7 \\
Gemini 3 Pro & 44/46 & 95.7 \\
GPT-5.2 & 23/46 & 50.0 \\
Qwen3 Coder Plus & 19/46 & 41.3 \\
DeepSeek v3.2 & 17/46 & 37.0 \\
Grok 4 Fast & 0/11$^{*}$ & 0.0 \\
\bottomrule
\end{tabular}
\caption{Prior knowledge of TC exploits. ``Familiar'' indicates model recognized the exploit and provided accurate details. $^{*}$Partial assessment (11/46 samples).}
\label{tab:knowledge_assessment}
\end{table}

Table~\ref{tab:knowledge_assessment} reveals substantial variation in prior knowledge. Llama and Claude/Gemini show near-complete familiarity (96--100\%), while DeepSeek and Qwen show lower rates (37--41\%). This differential explains some performance patterns: models with high familiarity may rely on memorized exploit signatures rather than code analysis.

\paragraph{Example Responses.} When familiar, models provide detailed, accurate descriptions. Claude on Nomad Bridge (ms\_tc\_001): ``\textit{August 2022...approximately \$190 million...a trusted root was incorrectly initialized to 0x00 (zero)...the bridge would approve any withdrawal request without proper verification.}'' When unfamiliar, models appropriately decline: ``\textit{I am not familiar with this specific security incident.}''
