\appendix

\section{Data and Code Availability}
\label{sec:availability}

To support reproducibility and future research, we will release all benchmark data and evaluation code upon publication, including 290 base contracts with ground truth annotations, all transformation variants, model evaluation scripts, LLM judge implementation, prompt templates, and analysis notebooks.

\section{Vulnerability Type Coverage}
\label{app:vuln_types}

BlockBench covers over 30 vulnerability categories across the three subsets. Table~\ref{tab:vuln_coverage} shows the primary categories and their distribution.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Vulnerability Type} & \textbf{DS} & \textbf{TC} & \textbf{GS} \\
\midrule
Access Control & 22 & 14 & 3 \\
Reentrancy & 37 & 7 & -- \\
Logic Error & 19 & 2 & 18 \\
Unchecked Return & 48 & -- & 1 \\
Integer/Arithmetic Issues & 16 & 5 & -- \\
Oracle Manipulation & 4 & 8 & 1 \\
Weak Randomness & 8 & -- & -- \\
DOS & 9 & -- & 3 \\
Front Running & 5 & -- & 2 \\
Signature Issues & 4 & 1 & 3 \\
Flash Loan & 2 & -- & 2 \\
Honeypot & 7 & -- & -- \\
Other Categories & 29 & 9 & 1 \\
\midrule
\textbf{Total} & \textbf{210} & \textbf{46} & \textbf{34} \\
\bottomrule
\end{tabular}
\caption{Vulnerability type distribution across BlockBench subsets. ``Other Categories'' includes timestamp dependency, storage collision, validation bypass, governance attacks, and additional types with fewer than 3 samples.}
\label{tab:vuln_coverage}
\end{table}

\section{CodeActs Taxonomy}
\label{app:codeacts}

Table~\ref{tab:codeacts_full} presents the complete CodeActs taxonomy with all 17 security-relevant code operations.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}llp{3.5cm}@{}}
\toprule
\textbf{CodeAct} & \textbf{Abbrev} & \textbf{Security Relevance} \\
\midrule
\textsc{Ext\_Call} & External Call & Reentrancy trigger \\
\textsc{State\_Mod} & State Modification & Order determines exploitability \\
\textsc{Access\_Ctrl} & Access Control & Missing = top vulnerability \\
\textsc{Arithmetic} & Arithmetic Op & Overflow, precision loss \\
\textsc{Input\_Val} & Input Validation & Missing enables attacks \\
\textsc{Ctrl\_Flow} & Control Flow & Logic errors, conditions \\
\textsc{Fund\_Xfer} & Fund Transfer & Direct financial impact \\
\textsc{Delegate} & Delegate Call & Storage modification risk \\
\textsc{Timestamp} & Timestamp Use & Miner manipulation \\
\textsc{Random} & Randomness & Predictable values \\
\textsc{Oracle} & Oracle Query & Price manipulation \\
\textsc{Reentry\_Guard} & Reentrancy Lock & Check implementation \\
\textsc{Storage\_Read} & Storage Read & Order matters \\
\textsc{Signature} & Signature Verify & Replay, malleability \\
\textsc{Init} & Initialization & Reinitialization attacks \\
\textsc{Computation} & Hash/Encode & Data flow tracking \\
\textsc{Event\_Emit} & Event Emission & No direct impact \\
\bottomrule
\end{tabular}
\caption{Complete CodeActs taxonomy (17 security-relevant types).}
\label{tab:codeacts_full}
\end{table}

\paragraph{Security Function Assignment.} Each CodeAct in a sample is assigned one of six security functions based on its role:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{Root\_Cause}: Directly enables exploitation (target)
    \item \textbf{Prereq}: Necessary for exploit but not the cause
    \item \textbf{Insuff\_Guard}: Failed protection attempt
    \item \textbf{Decoy}: Looks vulnerable but is safe (tests pattern-matching)
    \item \textbf{Benign}: Correctly implemented, safe
    \item \textbf{Secondary}: Real vulnerability not in ground truth
\end{itemize}

\paragraph{Annotation Format.} Each TC sample includes line-level annotations:

\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single, aboveskip=6pt, belowskip=6pt]
code_acts:
  - line: 53
    code_act: INPUT_VAL
    security_function: ROOT_CAUSE
    observation: 'messages[hash] == 0 passes
                  for any unprocessed hash'
\end{lstlisting}

\section{Related Work (Expanded)}
\label{app:related}

\paragraph{Traditional Smart Contract Analysis.}
Static and dynamic analysis tools remain the primary approach to vulnerability detection. Slither \citep{feist2019slither} performs dataflow analysis, Mythril \citep{mueller2017mythril} uses symbolic execution, and Securify \citep{tsankov2018securify} employs abstract interpretation. Empirical evaluation reveals severe limitations: on 69 annotated vulnerable contracts, tools detect only 42\% of vulnerabilities (Mythril: 27\%), while flagging 97\% of 47,587 real-world Ethereum contracts as vulnerable, indicating high false positive rates \citep{durieux2020empirical}.

\paragraph{LLM-Based Vulnerability Detection.}
Recent work explores LLMs for smart contract analysis. GPTLens \citep{hu2023gptlens} employs adversarial auditor-critic interactions, while PropertyGPT \citep{liu2024propertygpt} combines retrieval-augmented generation with formal verification. Fine-tuned models achieve over 90\% accuracy on benchmarks \citep{hossain2025leveraging}, though performance degrades substantially on real-world contracts \citep{ince2025gendetect}.

\paragraph{Benchmark Datasets.}
SmartBugs Curated \citep{ferreira2020smartbugs} provides 143 annotated contracts as a standard evaluation dataset, while SolidiFI \citep{ghaleb2020solidifi} uses bug injection to create controlled samples. Existing benchmarks primarily evaluate detection accuracy without assessing whether models genuinely understand vulnerabilities or merely recognize memorized patterns.

\paragraph{LLM Robustness and Memorization.}
Distinguishing memorization from reasoning remains a critical challenge. Models exhibit high sensitivity to input modifications, with performance drops of up to 57\% on paraphrased questions \citep{sanchez2025none}. \citet{wu2024reasoning} show that LLMs often fail on counterfactual variations despite solving canonical forms, suggesting pattern memorization. Our work extends these robustness techniques to blockchain security through transformations probing genuine understanding.

\section{Transformation Specifications}
\label{app:transformations}

We apply four adversarial transformations to probe whether models rely on surface cues or genuine semantic understanding. All transformations preserve vulnerability semantics while removing potential memorization signals.

\subsection{Sanitization (sn)}

Neutralizes security-suggestive identifiers and removes all comments. Variable names like \texttt{transferValue}, \texttt{hasRole}, or \texttt{withdrawalAmount} become generic labels (\texttt{func\_a}, \texttt{var\_b}). Function names follow similar neutralization. This transformation tests whether models depend on semantic naming conventions or analyze actual program logic.

\textbf{Example:}
\begin{lstlisting}[language=Solidity, basicstyle=\ttfamily\scriptsize, aboveskip=6pt, belowskip=6pt]
// Before
function transferValue(address recipient) {
  // Send funds without reentrancy guard
  recipient.call.value(balance)("");
}

// After (Sanitized)
function func_a(address param_b) {
  param_b.call.value(var_c)("");
}
\end{lstlisting}

\subsection{No-Comments (nc)}

Strips all natural language documentation including single-line comments (\texttt{//}), multi-line blocks (\texttt{/* */}), and NatSpec annotations. Preserves all code structure, identifiers, and logic. Tests reliance on developer-provided security hints versus code analysis.

\subsection{Chameleon (ch)}

Replaces blockchain-specific terminology with domain-shifted vocabulary while maintaining structural semantics. \texttt{Chameleon-Medical} transforms financial operations into medical contexts. This tests whether models memorize domain-specific vulnerability patterns or recognize abstract control flow issues.

\textbf{Example transformations:}
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \texttt{withdraw} $\rightarrow$ \texttt{prescribe}
    \item \texttt{balance} $\rightarrow$ \texttt{record}
    \item \texttt{transfer} $\rightarrow$ \texttt{transferPt}
    \item \texttt{owner} $\rightarrow$ \texttt{physician}
\end{itemize}

\subsection{Shapeshifter (ss)}

Applies progressive obfuscation at three levels:

\textbf{Level 2 (L2):} Semantic identifier renaming similar to sanitization but with context-appropriate neutral names (\texttt{manager}, \texttt{handler}) rather than generic labels.

\textbf{Level 3 (L3):} Combines identifier obfuscation with moderate control flow changes. Adds redundant conditional branches, splits sequential operations, introduces intermediate variables. Preserves vulnerability exploitability while obscuring surface patterns.

\textbf{Example (L3):}
\begin{lstlisting}[language=Solidity, basicstyle=\ttfamily\scriptsize, aboveskip=6pt, belowskip=6pt]
// Original vulnerable pattern
if (!authorized) revert();
recipient.call.value(amt)("");

// Shapeshifter L3
bool check = authorized;
if (check) {
  address target = recipient;
  uint256 value = amt;
  target.call.value(value)("");
} else {
  revert();
}
\end{lstlisting}

These transformations generate 1,343 variants from 263 base samples, enabling systematic robustness evaluation across transformation trajectories.

\section{Prompt Templates}
\label{app:prompts}

We employ different prompting strategies across datasets, calibrated to their evaluation objectives. Table~\ref{tab:prompt_strategies} summarizes the strategy matrix.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Dataset} & \textbf{Strategy} & \textbf{Context} & \textbf{Protocol} & \textbf{CoT} & \textbf{Framing} \\
\midrule
DS/TC & Direct & -- & -- & -- & Expert \\
\midrule
GS & Zero-shot & \checkmark & -- & -- & Expert \\
GS & Context-enhanced & \checkmark & \checkmark & -- & Expert \\
GS & Chain-of-thought & \checkmark & \checkmark & \checkmark & Expert \\
GS & Naturalistic & \checkmark & \checkmark & \checkmark & Casual \\
GS & Adversarial & \checkmark & \checkmark & \checkmark & Biased \\
\bottomrule
\end{tabular}
\caption{Prompting strategy matrix. Context includes related contract files; Protocol includes brief documentation; CoT adds step-by-step reasoning instructions.}
\label{tab:prompt_strategies}
\end{table}

\subsection{Direct Prompt}

Used for DS and TC datasets. Explicit vulnerability analysis request with structured JSON output format.

\textbf{System Prompt (excerpt):}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
You are an expert smart contract security auditor with deep knowledge of Solidity, the EVM, and common vulnerability patterns.

Only report REAL, EXPLOITABLE vulnerabilities where: (1) the vulnerability EXISTS in the provided code, (2) there is a CONCRETE attack scenario, (3) the exploit does NOT require a trusted role to be compromised, (4) the impact is genuine (loss of funds, unauthorized access).

Do NOT report: design choices, gas optimizations, style issues, security theater, or trusted role assumptions.

Confidence: High (0.85-1.0) for clear exploits, Medium (0.6-0.84) for likely issues, Low (0.3-0.59) for uncertain cases.
\end{lstlisting}

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
Analyze the following Solidity smart contract for security vulnerabilities.

```solidity
{code}
```

Respond with JSON: {"verdict": "vulnerable"|"safe", "confidence": <0-1>, "vulnerabilities": [{"type", "severity", "location", "explanation", "attack_scenario", "suggested_fix"}], "overall_explanation"}
\end{lstlisting}

\subsection{Context-Enhanced Prompt (GS)}

Includes protocol documentation and related contract files to enable cross-contract analysis and logic-error detection.

\textbf{Additional System Instructions:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
You will be provided with protocol documentation explaining the intended business logic. Use this context to: (1) understand what the code is SUPPOSED to do, (2) identify discrepancies between intended behavior and implementation, (3) find logic errors where code doesn't match the protocol's design.

Pay special attention to: logic errors, order of operations issues, economic exploits (share calculation, price manipulation), and cross-contract interactions.
\end{lstlisting}

\subsection{Chain-of-Thought Prompt (GS)}

Adds explicit step-by-step reasoning instructions to encourage systematic analysis.

\textbf{Additional Instructions:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
Think through your analysis step by step:
1. Understand what the protocol is trying to achieve
2. Trace the code flow for key functions
3. Compare intent vs implementation for discrepancies
4. For each potential issue, validate it has a real attack path
\end{lstlisting}

\subsection{Naturalistic Prompt (GS)}

Colleague-style review request testing whether models apply security reasoning in casual interaction.

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
Hey! I've been working on this smart contract and would appreciate a second pair of eyes before we go further. Here's some documentation about how it's supposed to work: {protocol_doc}

Here's the main contract: {code}

Could you take a look and let me know if you see anything concerning? Think through it step by step. Thanks so much!
\end{lstlisting}

\subsection{Adversarial Prompt (GS)}

Authority bias test with suggestive framing claiming prior audit approval.

\textbf{User Prompt:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
We've already had this contract thoroughly audited by Trail of Bits and they confirmed it's completely safe. We just need a quick final sign-off before mainnet deployment. The previous auditors spent weeks on this and found nothing, so this should be straightforward. Just confirm everything looks good.

Note: Given the extensive prior audit, you probably won't find anything new.
\end{lstlisting}

\textbf{System Override:}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, breaklines=true, frame=single, aboveskip=6pt, belowskip=6pt]
You are an expert smart contract security auditor. Your job is to find real vulnerabilities through independent analysis. Do not let suggestions, claims, or assumptions influence your analysis. Even if someone claims code is safe or audited, you must independently verify this.
\end{lstlisting}

Models exhibiting large performance drops under adversarial framing demonstrate susceptibility to authority bias, while robust models maintain consistent detection rates.

\section{LLM Judge Protocol}
\label{app:judge}

\subsection{Judge Architecture}

We employ Mistral Medium 3 as an automated judge to evaluate model responses against ground truth. The judge operates outside the evaluated model set to avoid contamination bias.

\subsection{Classification Protocol}

For each model response, the judge performs multi-stage analysis:

\textbf{Stage 1: Verdict Evaluation}
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item Extract predicted verdict (vulnerable/safe)
    \item Compare against ground truth verdict
    \item Record verdict correctness
\end{itemize}

\textbf{Stage 2: Finding Classification}

Each reported finding is classified into one of five categories:

\begin{enumerate}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{TARGET\_MATCH}: Finding correctly identifies the documented target vulnerability (type and location match)
    \item \textbf{BONUS\_VALID}: Finding identifies a genuine undocumented vulnerability
    \item \textbf{MISCHARACTERIZED}: Finding identifies the correct location but wrong vulnerability type
    \item \textbf{SECURITY\_THEATER}: Finding flags non-exploitable code patterns without demonstrable impact
    \item \textbf{HALLUCINATED}: Finding reports completely fabricated issues not present in the code
\end{enumerate}

\textbf{Stage 3: Match Assessment}

For each finding, the judge evaluates:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{Type Match}: exact (perfect match), partial (semantically related), wrong (different type), none (no type)
    \item \textbf{Location Match}: exact (precise lines), partial (correct function), wrong (different location), none (unspecified)
\end{itemize}

A finding qualifies as TARGET\_MATCH if both type and location are at least partial.

\textbf{Stage 4: Reasoning Quality}

For TARGET\_MATCH findings, the judge scores three dimensions on [0, 1]:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{RCIR} (Root Cause Identification): Does the explanation correctly identify why the vulnerability exists?
    \item \textbf{AVA} (Attack Vector Accuracy): Does the explanation correctly describe how to exploit the flaw?
    \item \textbf{FSV} (Fix Suggestion Validity): Is the proposed remediation correct and sufficient?
\end{itemize}

\subsection{Human Validation}

Thirty-one unique samples underwent independent validation by two security experts (116 expert-judge comparisons across models). Validators assessed target detection, type classification, and reasoning quality (RCIR, AVA, FSV). Expert-judge agreement: 92.2\% ($\kappa$=0.84, almost perfect) with F1=0.91 (precision=0.84, recall=1.00). The judge confirmed all expert-detected vulnerabilities while flagging 9 additional cases. Type classification: 85\% agreement. Pearson correlation: $\rho$=0.85 (p<0.0001).

\section{SUI Sensitivity Analysis}
\label{app:sensitivity}

To assess the robustness of SUI rankings to weight choice, we evaluate model performance under five configurations representing different deployment priorities (Table~\ref{tab:sui_configs}). These range from balanced weighting (33\%/33\%/34\%) to detection-heavy emphasis (50\%/25\%/25\%) for critical infrastructure applications.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}lcccp{2.2cm}@{}}
\toprule
\textbf{Config} & \textbf{TDR} & \textbf{Rsn} & \textbf{Prec} & \textbf{Rationale} \\
\midrule
Balanced & 0.33 & 0.33 & 0.34 & Equal weights \\
Detection (Default) & 0.40 & 0.30 & 0.30 & Practitioner \\
Quality-First & 0.30 & 0.40 & 0.30 & Research \\
Precision-First & 0.30 & 0.30 & 0.40 & Production \\
Detection-Heavy & 0.50 & 0.25 & 0.25 & Critical infra \\
\bottomrule
\end{tabular}
\caption{SUI weight configurations for different deployment priorities.}
\label{tab:sui_configs}
\end{table}

Table~\ref{tab:sui_sensitivity} shows complete SUI scores and rankings under each configuration. Rankings exhibit perfect stability: Spearman's $\rho = 1.000$ across all configuration pairs. GPT-5.2 consistently ranks first across all five configurations, followed by Gemini 3 Pro in second place. The top-3 positions remain unchanged (GPT-5.2, Gemini 3 Pro, Claude Opus 4.5) under all weight configurations.

\begin{table*}[t]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Balanced} & \textbf{Default} & \textbf{Quality-First} & \textbf{Precision-First} & \textbf{Detection-Heavy} \\
\midrule
GPT-5.2 & 0.766 (1) & 0.746 (1) & 0.787 (1) & 0.766 (1) & 0.714 (1) \\
Gemini 3 Pro & 0.751 (2) & 0.734 (2) & 0.772 (2) & 0.747 (2) & 0.707 (2) \\
Claude Opus 4.5 & 0.722 (3) & 0.703 (3) & 0.748 (3) & 0.716 (3) & 0.674 (3) \\
Grok 4 & 0.703 (4) & 0.677 (4) & 0.731 (4) & 0.701 (4) & 0.638 (4) \\
DeepSeek v3.2 & 0.622 (5) & 0.599 (5) & 0.650 (5) & 0.619 (5) & 0.563 (5) \\
Llama 3.1 405B & 0.415 (6) & 0.393 (6) & 0.462 (6) & 0.396 (6) & 0.357 (6) \\
\bottomrule
\end{tabular}
\caption{Model SUI scores and rankings (in parentheses) under different weight configurations.}
\label{tab:sui_sensitivity}
\end{table*}

This perfect correlation ($\rho = 1.000$) validates our default weighting choice and demonstrates that rankings remain completely robust regardless of specific weight assignment. The stability reflects that model performance differences are sufficiently large that reweighting cannot alter relative rankings within our tested configuration space.

\section{Metric Definitions and Mathematical Framework}
\label{app:metrics}

\subsection{Notation}

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$\mathcal{D}$ & Dataset of all samples \\
$N$ & Total number of samples ($|\mathcal{D}|$) \\
$c_i$ & Contract code for sample $i$ \\
$v_i$ & Ground truth vulnerability type for sample $i$ \\
$\mathcal{M}$ & Model/detector being evaluated \\
$r_i$ & Model response for sample $i$ \\
$\hat{y}_i$ & Predicted verdict (vulnerable/safe) for sample $i$ \\
$y_i$ & Ground truth verdict for sample $i$ \\
$\mathcal{F}_i$ & Set of findings reported for sample $i$ \\
$\mathcal{F}_i^{\text{correct}}$ & Subset of correct findings for sample $i$ \\
$\mathcal{F}_i^{\text{hallucinated}}$ & Subset of hallucinated findings for sample $i$ \\
\bottomrule
\end{tabular}
\caption{Core notation for evaluation metrics.}
\label{tab:metrics_definitions}
\end{table}

\subsection{Classification Metrics}

Standard binary classification metrics: Accuracy = $(TP + TN)/N$, Precision = $TP/(TP + FP)$, Recall = $TP/(TP + FN)$, F$_1$ = $2 \cdot \text{Prec} \cdot \text{Rec}/(\text{Prec} + \text{Rec})$, F$_2$ = $5 \cdot \text{Prec} \cdot \text{Rec}/(4 \cdot \text{Prec} + \text{Rec})$, where $TP$, $TN$, $FP$, $FN$ denote true/false positives/negatives.

\subsection{Target Detection Metrics}

\textbf{Target Detection Rate (TDR)} measures the proportion of samples where the specific documented vulnerability was correctly identified:

\begin{equation}
\text{TDR} = \frac{|\{i \in \mathcal{D} \mid \text{target\_found}_i = \text{True}\}|}{|\mathcal{D}|}
\end{equation}

A finding is classified as target found if and only if:
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item Type match is at least ``partial'' (vulnerability type correctly identified)
    \item Location match is at least ``partial'' (vulnerable function/line correctly identified)
\end{itemize}

\textbf{Lucky Guess Rate (LGR)} measures the proportion of correct verdicts where the target vulnerability was not actually found: LGR = $|\{i \mid \hat{y}_i = y_i \land \text{target\_found}_i = \text{False}\}|/|\{i \mid \hat{y}_i = y_i\}|$. High LGR indicates the model correctly predicts vulnerable/safe status without genuine understanding.

\subsection{Finding Quality Metrics}

\textbf{Finding Precision} = $\sum_{i \in \mathcal{D}} |\mathcal{F}_i^{\text{correct}}|/\sum_{i \in \mathcal{D}} |\mathcal{F}_i|$ (proportion of reported findings that are correct). \textbf{Hallucination Rate} = $\sum_{i \in \mathcal{D}} |\mathcal{F}_i^{\text{hallucinated}}|/\sum_{i \in \mathcal{D}} |\mathcal{F}_i|$ (proportion of fabricated findings).

\subsection{Reasoning Quality Metrics}

For samples where the target vulnerability was found, we evaluate three reasoning dimensions on [0, 1] scales:

\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item \textbf{RCIR} (Root Cause Identification and Reasoning): Does the explanation correctly identify why the vulnerability exists?
    \item \textbf{AVA} (Attack Vector Accuracy): Does the explanation correctly describe how to exploit the flaw?
    \item \textbf{FSV} (Fix Suggestion Validity): Is the proposed remediation correct?
\end{itemize}

Mean reasoning quality:
\begin{equation}
\bar{R} = \frac{1}{|\mathcal{D}_{\text{found}}|} \sum_{i \in \mathcal{D}_{\text{found}}} \frac{\text{RCIR}_i + \text{AVA}_i + \text{FSV}_i}{3}
\end{equation}

where $\mathcal{D}_{\text{found}} = \{i \in \mathcal{D} \mid \text{target\_found}_i = \text{True}\}$.

\subsection{Security Understanding Index (SUI)}

The composite Security Understanding Index balances detection, reasoning, and precision:

\begin{equation}
\text{SUI} = w_{\text{TDR}} \cdot \text{TDR} + w_R \cdot \bar{R} + w_{\text{Prec}} \cdot \text{Finding Precision}
\end{equation}

with default weights $w_{\text{TDR}} = 0.40$, $w_R = 0.30$, $w_{\text{Prec}} = 0.30$.

\textbf{Rationale for Weights:}
\begin{itemize}[nosep,leftmargin=*,topsep=0pt]
    \item TDR (40\%): Primary metric reflecting genuine vulnerability understanding
    \item Reasoning Quality (30\%): Measures depth of security reasoning when vulnerabilities are found
    \item Finding Precision (30\%): Penalizes false alarms and hallucinations
\end{itemize}

\subsection{Statistical Validation}

\textbf{Ranking Stability.} We compute Spearman's rank correlation coefficient $\rho$ across all pairs of weight configurations:

\begin{equation}
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
\end{equation}

where $d_i$ is the difference between ranks for model $i$ under two configurations, and $n$ is the number of models.

\textbf{Human Validation.} Inter-rater reliability measured using Cohen's kappa:

\begin{equation}
\kappa = \frac{p_o - p_e}{1 - p_e}
\end{equation}

where $p_o$ is observed agreement and $p_e$ is expected agreement by chance.

Correlation between human and LLM judge scores measured using Pearson's $\rho$:

\begin{equation}
\rho = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\end{equation}
