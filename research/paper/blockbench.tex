\section{BlockBench}

We introduce BlockBench, a benchmark for evaluating whether AI models genuinely understand smart contract vulnerabilities. The benchmark is designed to distinguish genuine security understanding from pattern memorization, comprising 290 vulnerable Solidity contracts with 322 transformation variants, spanning over 30 vulnerability categories (Appendix~\ref{app:vuln_types}).

Let $\mathcal{D}$ represent the dataset, where $\mathcal{D} = \{(c_i, v_i, m_i)\}_{i=1}^{290}$. Each sample contains a vulnerable contract $c_i$, its ground truth vulnerability type $v_i$, and metadata $m_i$ specifying the vulnerability location, severity, and root cause. We partition $\mathcal{D}$ into three disjoint subsets, $\mathcal{D} = \mathcal{D}_{\text{DS}} \cup \mathcal{D}_{\text{TC}} \cup \mathcal{D}_{\text{GS}}$, each targeting a distinct evaluation objective (Table~\ref{tab:dataset}).

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lrl@{}}
\toprule
\textbf{Subset} & \textbf{N} & \textbf{Sources} \\
\midrule
Difficulty Stratified (DS) & 210 & SmartBugs, DeFiVulnLabs \\
Temporal Contamination (TC) & 46 & Real-world exploits \\
Gold Standard (GS) & 34 & Code4rena, Spearbit \\
\bottomrule
\end{tabular}
\caption{BlockBench composition by subset and primary sources.}
\label{tab:dataset}
\end{table}

\paragraph{Difficulty Stratified.} $\mathcal{D}_{\text{DS}}$ draws from established vulnerability repositories including SmartBugs Curated \citep{ferreira2020smartbugs}, Trail of Bits' Not So Smart Contracts \citep{trailofbits2018}, and DeFiVulnLabs \citep{defivulnlabs2023}. Samples are stratified into four difficulty tiers based on detection complexity, with distribution $\{86, 81, 30, 13\}$ from Tier 1 (basic patterns) through Tier 4 (expert-level vulnerabilities requiring deep protocol knowledge). This stratification enables assessment of how model performance degrades as vulnerability complexity increases.

\paragraph{Temporal Contamination.} $\mathcal{D}_{\text{TC}}$ reconstructs 46 real-world DeFi exploits spanning 2016 to 2024, representing over \$1.65 billion in documented losses. Notable incidents include The DAO (\$60M, 2016), Nomad Bridge (\$190M, 2022), and Curve Vyper (\$70M, 2023). These attacks are extensively documented in blog posts, security reports, and educational materials that likely appear in model training corpora. To probe whether models genuinely understand these vulnerabilities or merely recognize them, we apply systematic transformations that preserve vulnerability semantics while removing surface cues (detailed in \S4).

\paragraph{Gold Standard.} $\mathcal{D}_{\text{GS}}$ derives from 34 professional security audit findings by Code4rena \citep{code4rena2025}, Spearbit \citep{spearbit2025}, and MixBytes \citep{mixbytes2025} disclosed after September 2025. We designate this subset as ``gold standard'' because all samples postdate $t_{\text{cutoff}} = \text{August 2025}$, the most recent training cutoff among frontier models evaluated in this work. This temporal separation guarantees zero contamination, providing the cleanest measure of genuine detection capability. The subset emphasizes logic errors (53\%) and includes 10 high-severity and 24 medium-severity findings.

These complementary subsets collectively enable rigorous assessment of both detection capability and the distinction between pattern memorization and genuine security understanding.
