\section*{Ethical Considerations}

BlockBench poses dual-use risks: adversarial transformations demonstrate methods that could suppress detection, while detailed vulnerability documentation may assist malicious actors. We justify public release on several grounds: adversarial robustness represents a fundamental requirement for security tools, malicious actors will discover these vulnerabilities regardless, and responsible disclosure enables proactive mitigation. All samples derive from already-disclosed vulnerabilities and public security audits, ensuring no novel exploit information is revealed. Practitioners should avoid over-reliance on imperfect tools, as false negatives create security gaps while false confidence may reduce manual review rigor.

\section*{Limitations and Future Work}

Our evaluation uses 180 original samples (DS $n$=100, TC $n$=46, GS $n$=34) with 322 TC transformation variants across seven models, yielding over 3,500 unique evaluations. We assess zero-shot prompting with five prompt protocols on GS, providing models only with contract code necessary to expose each vulnerability. In real audit settings, analysts often rely on additional semantic context such as protocol goals, intended invariants, expected economic behavior, and threat models.

The CodeAct analysis covers 46 samples with line-level annotations across three variants (MinimalSanitized, Trojan, Differential). While this enables fine-grained pattern matching analysis, broader annotation coverage would strengthen generalizability. Our LLM judge ensemble (GLM-4.7, MIMO-v2-Flash, Mistral-Large) achieves Fleiss' $\kappa$=0.78 with 92\% expert agreement, but automated evaluation may miss nuanced security reasoning.

Future work should explore retrieval-augmented analysis, expand CodeAct annotations across the full dataset, develop contamination-resistant methods using control-flow and data-flow representations, and explore hybrid LLM-verification architectures that integrate formal specifications with pattern recognition strengths.

\section*{AI Assistance}

Claude Sonnet 3.5 assisted with evaluation pipeline code and manuscript refinement. All research design, experimentation, and analysis were conducted by the authors.

\vspace{4em}